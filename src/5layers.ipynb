{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b21c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783679d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#не выполнять, если лэйблы проставлены, тест и трейн готовы\n",
    "from sklearn import preprocessing\n",
    "df = pd.read_csv(\"../data/mergedcleared1307.csv\", sep=';', names=['id', 'label'], dtype={'id': str, 'label': str})\n",
    "Label_encoder = preprocessing.LabelEncoder()\n",
    "Label_encoder.fit(df['id'])\n",
    "df['id'] = Label_encoder.fit_transform(df['id'])\n",
    "df.columns = ['labels', 'description']\n",
    "df.to_csv('/home/sincosxy/rab/dataset.csv', index=False)\n",
    "np.save('/home/sincosxy/rab/classes.npy', Label_encoder.classes_)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "train = df[:int(len(df)*0.8)]\n",
    "test =  df[int(len(df)*0.8):]\n",
    "train.to_csv('/home/sincosxy/rab/train.csv', index=False)\n",
    "test.to_csv('/home/sincosxy/rab/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f1615b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/sincosxy/.cache/huggingface/datasets/csv/default-c45fd002eecbe8f6/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23e66193aed46cb9495357269e60ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sincosxy/.cache/huggingface/datasets/csv/default-c45fd002eecbe8f6/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-33ea085ce5b04645.arrow\n",
      "Loading cached processed dataset at /home/sincosxy/.cache/huggingface/datasets/csv/default-c45fd002eecbe8f6/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-47070ad088e4822e.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "dataset = load_dataset('csv', data_files={'train': '/home/sincosxy/rab/train.csv', 'test': '/home/sincosxy/rab/test.csv'})\n",
    "dataset = dataset.map(lambda e: tokenizer(e['description'], truncation = True, max_length=100, padding='max_length'), batched=True)\n",
    "pytorch_style_columns = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']  # \n",
    "dataset = dataset.remove_columns(set(dataset['train'].features.keys()) - set(pytorch_style_columns))\n",
    "set(dataset['train'].features.keys())\n",
    "dataset.set_format(type='torch', columns=pytorch_style_columns, device='cuda') \n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset['train'], shuffle=True, batch_size=1)\n",
    "test_dataloader = DataLoader(dataset['test'], shuffle=False, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset['train'], shuffle=True, batch_size=12)\n",
    "test_dataloader = DataLoader(dataset['test'], shuffle=False, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017370cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "model_name = \"sberbank-ai/sbert_large_nlu_ru\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)#, num_labels=1024)\n",
    "#попробуем позже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dda5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import BertModel\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")\n",
    "        #BertModel.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\")\n",
    "          ### New layers:\n",
    "        self.linear1 = nn.Linear(1024, 8192)\n",
    "        self.linear2 = nn.Linear(8192, 2648)\n",
    "        #self.linear3 = nn.Linear(8192, 8192)\n",
    "        #self.linear4 = nn.Linear(8192, 8192)\n",
    "        #self.linear5 = nn.Linear(8192, 2648)\n",
    "        ## 3 is the number of classes in this example\n",
    "    def forward(self, ids, mask):\n",
    "        #sequence_output, pooled_output = self.bert(ids, attention_mask=mask)\n",
    "          # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "        linear1_output = self.linear1(self.bert(ids, attention_mask=mask)[0][:,0,:].view(-1,1024)) ## extract the 1st token's embeddings\n",
    "        #linear1_output = self.linear1(sequence_output)\n",
    "        linear2_output = self.linear2(linear1_output)\n",
    "        #linear3_output = self.linear3(linear2_output)\n",
    "        #linear4_output = self.linear4(linear3_output)\n",
    "        #linear5_output = self.linear5(linear4_output)\n",
    "        return linear2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e513b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\")\n",
    "model = CustomBERTModel() # You can pass the parameters if required to have more flexible model\n",
    "model.to(device)\n",
    "#model.to(torch.device(\"cpu\")) ## can be gpu\n",
    "criterion = nn.CrossEntropyLoss() ## If required define your own criterion\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00495d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = next(iter(train_dataloader))\n",
    "batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = model(batch1['input_ids'].to(torch.device('cpu')), batch1['attention_mask'].to(torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca67bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c550057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████▎        | 146665/186380 [8:08:47<2:12:01,  5.01it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(train_dataloader): ## If you have a DataLoader()  object to get the data.\n",
    "        #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        #outputs = model(**batch)\n",
    "        #data = batch[0]\n",
    "        #targets = batch[1] ## assuming that data loader returns a tuple of data and its targets\n",
    "        \n",
    "        optimizer.zero_grad()   \n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "        #encoding = tokenizer.batch_encode_plus(data, return_tensors='pt', padding=True, truncation=True,max_length=50, add_special_tokens = True)\n",
    "        #outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        outputs =  torch.nn.functional.softmax(outputs, dim=1)#F.log_softmax(outputs, dim=1)\n",
    "        # torch.nn.functional.\n",
    "        #input_ids = encoding['input_ids']\n",
    "        #attention_mask = encoding['attention_mask']\n",
    "        loss = criterion(outputs, batch['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799638a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-6)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"constant_with_warmup\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=2000,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f22b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98be2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(iter(train_dataloader))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849bd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0.\n",
    "show_train_loss_every_num_epoch = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(40*'-', '\\nepoch', epoch+1)\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        model.train()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        #f i%int(len(train_dataloader)*show_train_loss_every_num_epoch)==int(len(train_dataloader)*show_train_loss_every_num_epoch)-1:\n",
    "    print(f'train loss [{i*100/len(train_dataloader):.2f}%]: {np.array(losses).mean():.3f}')\n",
    "    losses = []\n",
    "    print('\\nvalidating')\n",
    "\n",
    "    f1 = load_metric('f1')\n",
    "    acc = load_metric('accuracy')\n",
    "    precision = load_metric('precision')\n",
    "    recall = load_metric('recall')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "                \n",
    "        for batch in tqdm(test_dataloader):\n",
    "        ##for batch in test_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            f1.add_batch(predictions=predictions, references=batch[\"labels\"])    \n",
    "            acc.add_batch(predictions=predictions, references=batch[\"labels\"])    \n",
    "            precision.add_batch(predictions=predictions, references=batch[\"labels\"])    \n",
    "            recall.add_batch(predictions=predictions, references=batch[\"labels\"])    \n",
    "\n",
    "        print('weighted summary:')\n",
    "        print('Test acc:', acc.compute()['accuracy'])\n",
    "        print('Test precision:', precision.compute(average = 'weighted')['precision'])\n",
    "        print('Test recall:', recall.compute(average = 'weighted')['recall'])\n",
    "        f1_weighted = f1.compute(average = 'weighted')['f1']\n",
    "        print('Test f1:', f1_weighted, '\\n')\n",
    "\n",
    "        if f1_weighted > best_f1:\n",
    "            best_f1 = f1_weighted\n",
    "            model.save_pretrained(\"../../best_model2705\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "Label_encoder = preprocessing.LabelEncoder()\n",
    "Label_encoder.classes_ = np.load('./classes.npy', allow_pickle=True)\n",
    "\n",
    "true = []\n",
    "preds = []\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(test_dataloader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    true += batch[\"labels\"].detach().cpu().numpy().tolist()\n",
    "    preds += predictions.detach().cpu().numpy().tolist()\n",
    "\n",
    "print(classification_report(Label_encoder.inverse_transform(true), Label_encoder.inverse_transform(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('покрышки', truncation = True, max_length=100, padding='max_length', return_tensors=\"pt\")\n",
    "#batch = {k: torch.tensor(v).to(device) for k, v in txt.items()}\n",
    "model.to(torch.device('cpu'))\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "predicted_class_id = logits.argmax().item()\n",
    "#Label_encoder.inverse_transform(predicted_class_id)\n",
    "a = []\n",
    "a.append(predicted_class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f5252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_encoder.inverse_transform([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2efb846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(text):\n",
    "    model.to(torch.device('cpu'))\n",
    "    inputs = tokenizer(text, truncation = True, max_length=100, padding='max_length', return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "        return Label_encoder.inverse_transform([predicted_class_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_class(\"Велосипед\"), predict_class(\"велосипед\"), predict_class(\"Велосипедов\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
