{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b21c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased-sentence and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\yeril\\AppData\\Local\\Temp\\ipykernel_20504\\4145744160.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test.drop(['id', 'g_id'], axis=1, inplace=True)\n",
      "C:\\Users\\yeril\\AppData\\Local\\Temp\\ipykernel_20504\\4145744160.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train.drop(['id', 'g_id'], axis=1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/yeril/.cache/huggingface/datasets/csv/default-16101415d53253b2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2061.08it/s]\n",
      "Extracting data files: 100%|████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 400.12it/s]\n",
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/yeril/.cache/huggingface/datasets/csv/default-16101415d53253b2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 83.95it/s]\n",
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "model_name = \"DeepPavlov/rubert-base-cased-sentence\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=660)\n",
    "model.to(device)\n",
    "\n",
    "df = pd.read_csv(\"../data/mergedcleared1307.csv\", sep=';', names=['id', 'label'], dtype={'id': str, 'label': str})\n",
    "df = df[df['label'].notnull()]\n",
    "df['g_id'] = df.id.str.slice(start=0, stop=4)\n",
    "Label_encoder = preprocessing.LabelEncoder()\n",
    "Label_encoder.fit(df['g_id'])\n",
    "df['g_id_enc'] = Label_encoder.fit_transform(df['g_id'])\n",
    "df.columns = ['id', 'description', 'g_id', 'labels']\n",
    "#data = df.drop(['id', 'g_id'], axis=1, inplace=False)\n",
    "df.to_csv('../sets/df-id-gid-label.csv', index=False)\n",
    "#data.to_csv('../sets/df-label.csv', index=False)\n",
    "np.save('../sets/classes4d.npy', Label_encoder.classes_)\n",
    "data = df.sample(frac=1).reset_index(drop=True)\n",
    "train = data[:int(len(df)*0.8)]\n",
    "test =  data[int(len(df)*0.8):]\n",
    "train.to_csv('../sets/train_with_ids.csv', index=False)\n",
    "test.to_csv('../sets/test_with_ids.csv', index=False)\n",
    "test.drop(['id', 'g_id'], axis=1, inplace=True)\n",
    "train.drop(['id', 'g_id'], axis=1, inplace=True)\n",
    "train.to_csv('../sets/train.csv', index=False)\n",
    "test.to_csv('../sets/test.csv', index=False)\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': '../sets/train.csv', 'test': '../sets/test.csv'})\n",
    "dataset = dataset.map(lambda e: tokenizer(e['description'], truncation = True, max_length=100, padding='max_length'), batched=True)\n",
    "pytorch_style_columns = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']  # \n",
    "dataset = dataset.remove_columns(set(dataset['train'].features.keys()) - set(pytorch_style_columns))\n",
    "dataset.set_format(type='torch', columns=pytorch_style_columns, device='cuda')\n",
    "\n",
    "train_dataloader = DataLoader(dataset['train'], shuffle=True, batch_size=4)\n",
    "test_dataloader = DataLoader(dataset['test'], shuffle=False, batch_size=4)\n",
    "#если есть видеопамять около 10гб, можно поставить 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799638a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-6)\n",
    "num_epochs = 15\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"constant_with_warmup\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=2000,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849bd9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- \n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23298/23298 [3:54:51<00:00,  1.65it/s]\n",
      "C:\\Users\\yeril\\AppData\\Local\\Temp\\ipykernel_20504\\180394090.py:25: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  f1 = load_metric('f1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [100.00%]: 4.249\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5825/5825 [10:30<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.5494463044038115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.44686225834453636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test recall: 0.5494463044038115\n",
      "Test f1: 0.45306600284587395 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23298/23298 [3:51:45<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [100.00%]: 2.247\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5825/5825 [10:31<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.7196111254184908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.6512836849254209\n",
      "Test recall: 0.7196111254184908\n",
      "Test f1: 0.6583117883751676 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23298/23298 [3:51:42<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [100.00%]: 1.405\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5825/5825 [10:30<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.809017941454202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.769648826402909\n",
      "Test recall: 0.809017941454202\n",
      "Test f1: 0.770145492224197 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23298/23298 [3:51:44<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [100.00%]: 0.943\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5825/5825 [10:29<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.8669842904970384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.8519589276486657\n",
      "Test recall: 0.8669842904970384\n",
      "Test f1: 0.8454398400865679 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23298/23298 [3:53:49<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [100.00%]: 0.658\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5825/5825 [10:29<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9052279165593613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.8988398246904225\n",
      "Test recall: 0.9052279165593613\n",
      "Test f1: 0.8940048680448462 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23298/23298 [3:55:40<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [100.00%]: 0.468\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5825/5825 [10:50<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9258949266031419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.9236027187844431\n",
      "Test recall: 0.9258949266031419\n",
      "Test f1: 0.9197142948325676 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23298/23298 [3:56:24<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [100.00%]: 0.339\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5825/5825 [10:30<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9388359515838269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.9377031747620691\n",
      "Test recall: 0.9388359515838269\n",
      "Test f1: 0.9356008178723766 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23298/23298 [3:53:04<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [100.00%]: 0.248\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5825/5825 [10:29<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9457464160013735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.9472029017705119\n",
      "Test recall: 0.9457464160013735\n",
      "Test f1: 0.9441380244831442 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23298/23298 [3:52:51<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [100.00%]: 0.185\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5825/5825 [10:31<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9495021031848228\n",
      "Test precision: 0.9512916947010742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test recall: 0.9495021031848228\n",
      "Test f1: 0.9488496749252348 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23298/23298 [3:52:48<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [100.00%]: 0.141\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5825/5825 [10:31<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9519057429822302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.9525739812292136\n",
      "Test recall: 0.9519057429822302\n",
      "Test f1: 0.9513583800832051 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23298/23298 [3:56:19<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [100.00%]: 0.109\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5825/5825 [10:47<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9532792514378917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.9542291150877494\n",
      "Test recall: 0.9532792514378917\n",
      "Test f1: 0.9529606409473039 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████████████▌                                                 | 7539/23298 [1:15:26<2:37:42,  1.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_f1 = 0.\n",
    "show_train_loss_every_num_epoch = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(40*'-', '\\nepoch', epoch+1)\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        model.train()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        #f i%int(len(train_dataloader)*show_train_loss_every_num_epoch)==int(len(train_dataloader)*show_train_loss_every_num_epoch)-1:\n",
    "    print(f'train loss [{i*100/len(train_dataloader):.2f}%]: {np.array(losses).mean():.3f}')\n",
    "    losses = []\n",
    "    print('\\nvalidating')\n",
    "\n",
    "    f1 = load_metric('f1')\n",
    "    acc = load_metric('accuracy')\n",
    "    precision = load_metric('precision')\n",
    "    recall = load_metric('recall')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "                \n",
    "        for batch in tqdm(test_dataloader):\n",
    "        ##for batch in test_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            f1.add_batch(predictions=predictions, references=batch[\"labels\"])    \n",
    "            acc.add_batch(predictions=predictions, references=batch[\"labels\"])    \n",
    "            precision.add_batch(predictions=predictions, references=batch[\"labels\"])    \n",
    "            recall.add_batch(predictions=predictions, references=batch[\"labels\"])    \n",
    "\n",
    "        print('weighted summary:')\n",
    "        print('Test acc:', acc.compute()['accuracy'])\n",
    "        print('Test precision:', precision.compute(average = 'weighted')['precision'])\n",
    "        print('Test recall:', recall.compute(average = 'weighted')['recall'])\n",
    "        f1_weighted = f1.compute(average = 'weighted')['f1']\n",
    "        print('Test f1:', f1_weighted, '\\n')\n",
    "\n",
    "        if f1_weighted > best_f1:\n",
    "            best_f1 = f1_weighted\n",
    "            model.save_pretrained(\"../../../DeepPavlov_model\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8404d0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5825/5825 [10:24<00:00,  9.33it/s]\n",
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0207       0.94      1.00      0.97        17\n",
      "        0304       1.00      0.91      0.95        11\n",
      "        0305       0.95      1.00      0.98        42\n",
      "        0604       1.00      1.00      1.00        11\n",
      "        0701       1.00      1.00      1.00        12\n",
      "        0703       1.00      1.00      1.00        18\n",
      "        0711       0.97      1.00      0.99        35\n",
      "        0712       1.00      1.00      1.00        13\n",
      "        0713       1.00      1.00      1.00        21\n",
      "        0801       0.95      1.00      0.97        19\n",
      "        0802       1.00      1.00      1.00        47\n",
      "        0803       1.00      1.00      1.00        22\n",
      "        0804       0.97      1.00      0.99        38\n",
      "        0805       1.00      1.00      1.00        69\n",
      "        0806       1.00      1.00      1.00        13\n",
      "        0808       1.00      1.00      1.00       139\n",
      "        0813       0.97      1.00      0.99        34\n",
      "        0901       1.00      1.00      1.00        39\n",
      "        0902       0.99      1.00      0.99        90\n",
      "        0910       1.00      0.93      0.96        27\n",
      "        1018       0.00      0.00      0.00         1\n",
      "        1104       1.00      1.00      1.00        14\n",
      "        1202       1.00      1.00      1.00        14\n",
      "        1207       1.00      1.00      1.00        26\n",
      "        1212       0.83      1.00      0.91         5\n",
      "        1301       0.86      0.80      0.83        15\n",
      "        1302       0.92      0.92      0.92        13\n",
      "        1401       0.96      1.00      0.98        24\n",
      "        1513       1.00      1.00      1.00        18\n",
      "        1515       0.00      0.00      0.00         3\n",
      "        1518       0.94      1.00      0.97        16\n",
      "        1702       1.00      0.92      0.96        12\n",
      "        1704       1.00      0.98      0.99       154\n",
      "        1806       1.00      1.00      1.00        14\n",
      "        1905       1.00      1.00      1.00         6\n",
      "        2001       0.96      0.99      0.97        76\n",
      "        2003       1.00      1.00      1.00        29\n",
      "        2005       1.00      0.95      0.98        66\n",
      "        2006       1.00      1.00      1.00        10\n",
      "        2007       1.00      0.92      0.96        25\n",
      "        2008       0.98      1.00      0.99        96\n",
      "        2009       1.00      1.00      1.00        15\n",
      "        2101       1.00      1.00      1.00        13\n",
      "        2103       0.92      0.96      0.94        25\n",
      "        2105       1.00      1.00      1.00        43\n",
      "        2106       0.94      0.88      0.91       105\n",
      "        2202       0.61      0.92      0.73        12\n",
      "        2203       1.00      1.00      1.00        16\n",
      "        2208       1.00      1.00      1.00        10\n",
      "        2304       0.94      1.00      0.97        17\n",
      "        2306       0.93      0.93      0.93        15\n",
      "        2309       1.00      1.00      1.00        16\n",
      "        2504       1.00      0.94      0.97        16\n",
      "        2517       1.00      0.93      0.97        15\n",
      "        2519       0.94      1.00      0.97        15\n",
      "        2526       1.00      0.92      0.96        12\n",
      "        2702       1.00      1.00      1.00        18\n",
      "        2710       0.96      0.92      0.94        84\n",
      "        2804       1.00      1.00      1.00        18\n",
      "        2811       0.73      0.69      0.71        16\n",
      "        2812       0.00      0.00      0.00         2\n",
      "        2813       0.94      1.00      0.97        16\n",
      "        2818       0.96      1.00      0.98        26\n",
      "        2821       0.93      1.00      0.97        14\n",
      "        2823       1.00      1.00      1.00         9\n",
      "        2830       1.00      0.94      0.97        17\n",
      "        2833       0.97      1.00      0.98        30\n",
      "        2835       1.00      1.00      1.00        21\n",
      "        2836       1.00      1.00      1.00        11\n",
      "        2839       0.83      0.91      0.87        11\n",
      "        2840       1.00      1.00      1.00        13\n",
      "        2842       0.88      0.82      0.85        17\n",
      "        2846       1.00      1.00      1.00        12\n",
      "        2847       0.94      0.94      0.94        16\n",
      "        2902       1.00      0.88      0.93        16\n",
      "        2903       1.00      1.00      1.00         8\n",
      "        2905       0.98      1.00      0.99        46\n",
      "        2907       0.94      0.97      0.95        30\n",
      "        2908       0.90      0.69      0.78        13\n",
      "        2909       0.92      0.79      0.85        28\n",
      "        2912       0.88      0.97      0.92        36\n",
      "        2914       0.89      0.89      0.89        27\n",
      "        2915       0.95      0.98      0.96        82\n",
      "        2916       1.00      0.88      0.94        26\n",
      "        2917       1.00      0.83      0.91        18\n",
      "        2918       0.96      0.91      0.93        55\n",
      "        2919       0.94      1.00      0.97        16\n",
      "        2921       0.82      0.86      0.84        37\n",
      "        2922       0.84      0.90      0.87        30\n",
      "        2923       0.93      0.81      0.87        16\n",
      "        2924       0.44      0.88      0.59        41\n",
      "        2926       1.00      0.57      0.73        14\n",
      "        2927       0.92      0.79      0.85        14\n",
      "        2930       0.65      0.61      0.63        18\n",
      "        2931       1.00      0.50      0.67         2\n",
      "        2932       0.76      0.83      0.79        23\n",
      "        2933       0.90      0.86      0.88       130\n",
      "        2934       0.70      0.83      0.76        59\n",
      "        2935       0.74      0.78      0.76        18\n",
      "        2936       0.95      0.88      0.91        48\n",
      "        2939       1.00      0.79      0.88        14\n",
      "        2941       1.00      0.70      0.82        30\n",
      "        3003       0.86      0.92      0.89        13\n",
      "        3005       1.00      0.92      0.96        12\n",
      "        3006       1.00      0.86      0.92        14\n",
      "        3204       0.90      0.92      0.91        61\n",
      "        3206       0.77      0.91      0.83        33\n",
      "        3208       0.89      0.91      0.90        35\n",
      "        3209       0.95      0.83      0.88        46\n",
      "        3212       0.94      0.65      0.77        23\n",
      "        3213       1.00      0.88      0.94        33\n",
      "        3214       0.90      0.93      0.92        60\n",
      "        3215       1.00      0.98      0.99        53\n",
      "        3301       0.98      0.97      0.97        60\n",
      "        3302       1.00      0.75      0.86        12\n",
      "        3304       0.98      0.95      0.97        87\n",
      "        3305       0.99      0.99      0.99       657\n",
      "        3306       1.00      0.97      0.99        36\n",
      "        3307       0.94      0.97      0.95       248\n",
      "        3401       0.92      0.90      0.91        61\n",
      "        3402       0.97      0.90      0.93        68\n",
      "        3403       0.83      0.93      0.88        43\n",
      "        3404       0.91      0.94      0.92        32\n",
      "        3405       0.92      0.92      0.92        36\n",
      "        3406       0.97      1.00      0.98        28\n",
      "        3407       0.90      1.00      0.95        26\n",
      "        3505       1.00      0.96      0.98        26\n",
      "        3506       0.96      0.88      0.92        25\n",
      "        3703       0.94      1.00      0.97        17\n",
      "        3801       0.94      1.00      0.97        31\n",
      "        3802       0.88      0.94      0.91        31\n",
      "        3808       1.00      0.99      1.00       163\n",
      "        3810       0.76      0.95      0.84        20\n",
      "        3812       1.00      0.87      0.93        15\n",
      "        3815       1.00      0.80      0.89        20\n",
      "        3816       0.87      0.93      0.90        14\n",
      "        3822       0.91      0.94      0.92        32\n",
      "        3823       1.00      0.98      0.99        41\n",
      "        3824       0.78      0.90      0.83        58\n",
      "        3901       0.95      0.96      0.95        92\n",
      "        3902       0.97      0.97      0.97        29\n",
      "        3903       1.00      0.71      0.83        14\n",
      "        3904       0.94      0.94      0.94        32\n",
      "        3906       0.77      0.77      0.77        13\n",
      "        3907       0.97      0.98      0.97       299\n",
      "        3908       0.88      0.82      0.85        17\n",
      "        3909       0.95      0.96      0.95       115\n",
      "        3910       1.00      0.81      0.90        16\n",
      "        3911       0.79      0.61      0.69        18\n",
      "        3912       1.00      0.96      0.98        91\n",
      "        3913       0.67      0.83      0.74        12\n",
      "        3916       0.95      0.94      0.94        80\n",
      "        3917       0.96      0.96      0.96       258\n",
      "        3918       0.98      1.00      0.99        47\n",
      "        3919       0.95      0.96      0.96       129\n",
      "        3920       0.98      0.94      0.96       373\n",
      "        3921       0.88      0.93      0.90       287\n",
      "        3922       0.97      0.93      0.95        73\n",
      "        3923       0.89      0.93      0.91       191\n",
      "        3924       0.97      0.97      0.97       115\n",
      "        3925       0.89      0.88      0.88        82\n",
      "        3926       0.93      0.96      0.94       467\n",
      "        4002       0.92      1.00      0.96        11\n",
      "        4005       1.00      0.77      0.87        22\n",
      "        4006       1.00      0.79      0.88        14\n",
      "        4008       0.91      0.87      0.89        79\n",
      "        4009       0.95      0.97      0.96       144\n",
      "        4010       0.98      0.98      0.98       175\n",
      "        4011       1.00      1.00      1.00      1863\n",
      "        4012       0.91      0.97      0.94        33\n",
      "        4013       1.00      1.00      1.00        61\n",
      "        4014       0.97      0.97      0.97        30\n",
      "        4015       0.83      1.00      0.90        19\n",
      "        4016       0.88      0.94      0.91       250\n",
      "        4017       1.00      0.71      0.83         7\n",
      "        4201       1.00      0.93      0.96        14\n",
      "        4202       0.97      0.96      0.96       365\n",
      "        4203       0.95      0.87      0.91        23\n",
      "        4205       1.00      1.00      1.00        15\n",
      "        4303       1.00      1.00      1.00        13\n",
      "        4304       0.83      1.00      0.90        19\n",
      "        4405       1.00      0.96      0.98        28\n",
      "        4407       0.00      0.00      0.00         1\n",
      "        4408       0.91      1.00      0.95        10\n",
      "        4409       0.96      0.98      0.97        48\n",
      "        4411       0.99      0.97      0.98        77\n",
      "        4412       1.00      1.00      1.00        13\n",
      "        4417       1.00      1.00      1.00        18\n",
      "        4418       0.94      0.94      0.94        81\n",
      "        4419       0.98      0.97      0.97        60\n",
      "        4420       0.97      0.86      0.91        36\n",
      "        4421       0.87      0.85      0.86        53\n",
      "        4503       1.00      1.00      1.00        18\n",
      "        4504       0.91      0.91      0.91        11\n",
      "        4601       0.87      0.98      0.92        56\n",
      "        4602       0.99      0.94      0.97        89\n",
      "        4802       0.95      1.00      0.98        20\n",
      "        4804       1.00      1.00      1.00        14\n",
      "        4805       0.93      1.00      0.96        13\n",
      "        4806       0.97      1.00      0.99        37\n",
      "        4808       0.93      1.00      0.96        27\n",
      "        4810       1.00      0.83      0.91        24\n",
      "        4811       0.83      0.84      0.83        69\n",
      "        4812       0.89      0.73      0.80        11\n",
      "        4817       0.94      0.94      0.94        16\n",
      "        4818       0.81      1.00      0.89        25\n",
      "        4819       0.95      0.95      0.95        99\n",
      "        4820       0.96      0.96      0.96       113\n",
      "        4821       0.89      0.96      0.92        51\n",
      "        4823       0.86      0.89      0.87        82\n",
      "        4901       0.91      0.91      0.91        11\n",
      "        4903       1.00      1.00      1.00        15\n",
      "        4905       1.00      1.00      1.00        10\n",
      "        4908       1.00      0.82      0.90        17\n",
      "        4909       0.89      0.89      0.89         9\n",
      "        4910       1.00      1.00      1.00        19\n",
      "        4911       0.90      0.90      0.90        50\n",
      "        5007       0.96      1.00      0.98        26\n",
      "        5022       1.00      0.50      0.67         4\n",
      "        5058       0.00      0.00      0.00         1\n",
      "        5105       0.95      1.00      0.97        18\n",
      "        5204       1.00      1.00      1.00         8\n",
      "        5206       1.00      1.00      1.00         8\n",
      "        5208       0.93      0.95      0.94       132\n",
      "        5209       0.92      0.90      0.91        78\n",
      "        5210       0.89      0.83      0.86        29\n",
      "        5211       0.79      0.92      0.85        12\n",
      "        5305       0.93      0.93      0.93        14\n",
      "        5310       1.00      1.00      1.00        16\n",
      "        5311       0.89      0.73      0.80        11\n",
      "        5401       0.95      0.95      0.95        41\n",
      "        5402       0.96      1.00      0.98        53\n",
      "        5404       0.94      1.00      0.97        32\n",
      "        5407       0.98      0.97      0.97       206\n",
      "        5503       1.00      0.83      0.90        23\n",
      "        5508       0.96      0.98      0.97        55\n",
      "        5511       1.00      1.00      1.00         9\n",
      "        5512       0.82      0.93      0.87        29\n",
      "        5513       0.93      0.88      0.90        16\n",
      "        5514       1.00      0.95      0.97        37\n",
      "        5516       1.00      0.89      0.94        28\n",
      "        5601       1.00      0.98      0.99        41\n",
      "        5602       0.86      0.93      0.89        27\n",
      "        5603       0.95      0.95      0.95       155\n",
      "        5604       0.81      0.93      0.87        14\n",
      "        5607       0.91      0.94      0.92       135\n",
      "        5608       0.89      0.98      0.93        42\n",
      "        5609       0.59      0.62      0.61        16\n",
      "        5701       1.00      1.00      1.00        16\n",
      "        5702       0.97      0.94      0.95        32\n",
      "        5703       0.97      0.91      0.94        34\n",
      "        5704       1.00      0.92      0.96        12\n",
      "        5705       0.85      0.92      0.88        36\n",
      "        5801       0.98      0.96      0.97        51\n",
      "        5804       1.00      1.00      1.00        40\n",
      "        5806       0.92      0.91      0.92        54\n",
      "        5807       0.94      1.00      0.97        29\n",
      "        5808       0.93      0.82      0.87        34\n",
      "        5810       1.00      1.00      1.00        13\n",
      "        5901       1.00      0.94      0.97        18\n",
      "        5903       0.85      0.93      0.89        89\n",
      "        5906       0.95      1.00      0.98        20\n",
      "        5907       1.00      0.62      0.76        13\n",
      "        5908       1.00      0.75      0.86        12\n",
      "        5911       0.93      0.84      0.88        61\n",
      "        6001       0.98      0.96      0.97        51\n",
      "        6002       1.00      0.88      0.93         8\n",
      "        6004       1.00      0.85      0.92        13\n",
      "        6005       1.00      0.94      0.97        16\n",
      "        6006       0.92      1.00      0.96        23\n",
      "        6101       1.00      1.00      1.00        37\n",
      "        6102       1.00      0.80      0.89        10\n",
      "        6103       0.93      0.94      0.94        54\n",
      "        6104       0.93      0.97      0.95        70\n",
      "        6105       1.00      1.00      1.00        13\n",
      "        6106       1.00      1.00      1.00        17\n",
      "        6107       1.00      0.93      0.96        27\n",
      "        6108       0.75      0.86      0.80         7\n",
      "        6109       0.94      1.00      0.97        30\n",
      "        6110       0.86      1.00      0.92        42\n",
      "        6111       1.00      0.67      0.80        18\n",
      "        6113       1.00      0.94      0.97        16\n",
      "        6115       0.96      0.99      0.97        93\n",
      "        6116       1.00      0.94      0.97        71\n",
      "        6117       0.91      0.98      0.95        53\n",
      "        6201       1.00      0.97      0.98        31\n",
      "        6202       0.91      1.00      0.95        20\n",
      "        6203       0.98      0.97      0.97       126\n",
      "        6204       0.95      0.98      0.96       160\n",
      "        6206       0.94      1.00      0.97        16\n",
      "        6208       1.00      0.87      0.93        23\n",
      "        6209       1.00      0.71      0.83        21\n",
      "        6210       0.97      0.92      0.95        39\n",
      "        6211       0.93      1.00      0.96        37\n",
      "        6212       1.00      1.00      1.00        20\n",
      "        6213       0.93      0.90      0.91        29\n",
      "        6214       0.95      0.96      0.95        76\n",
      "        6216       0.88      0.94      0.91        31\n",
      "        6217       0.94      0.94      0.94        18\n",
      "        6301       0.90      1.00      0.95        36\n",
      "        6302       0.96      0.98      0.97       102\n",
      "        6303       1.00      0.89      0.94        18\n",
      "        6304       0.98      0.83      0.90        60\n",
      "        6305       0.95      0.97      0.96        78\n",
      "        6306       0.95      0.88      0.91        60\n",
      "        6307       0.92      0.95      0.93       133\n",
      "        6308       1.00      1.00      1.00         8\n",
      "        6401       1.00      0.92      0.96        12\n",
      "        6402       0.97      0.96      0.96        70\n",
      "        6403       1.00      0.99      1.00       139\n",
      "        6404       1.00      1.00      1.00        29\n",
      "        6406       0.98      0.97      0.97        98\n",
      "        6504       0.85      1.00      0.92        17\n",
      "        6505       0.94      0.85      0.89        39\n",
      "        6506       0.91      0.91      0.91        43\n",
      "        6507       1.00      0.94      0.97        16\n",
      "        6601       1.00      1.00      1.00        84\n",
      "        6603       1.00      1.00      1.00        16\n",
      "        6701       1.00      1.00      1.00         8\n",
      "        6702       1.00      0.97      0.99        37\n",
      "        6703       0.85      1.00      0.92        11\n",
      "        6704       1.00      0.80      0.89        15\n",
      "        6801       0.93      1.00      0.97        14\n",
      "        6802       0.98      0.97      0.97        96\n",
      "        6804       0.97      0.94      0.95       124\n",
      "        6805       0.95      0.92      0.93        62\n",
      "        6806       0.87      0.87      0.87        15\n",
      "        6809       1.00      1.00      1.00        19\n",
      "        6810       0.98      1.00      0.99        41\n",
      "        6812       0.91      0.91      0.91        33\n",
      "        6813       0.96      0.88      0.92        52\n",
      "        6815       0.00      0.00      0.00         2\n",
      "        6901       0.76      0.76      0.76        17\n",
      "        6902       0.79      0.85      0.81        13\n",
      "        6903       0.78      0.70      0.74        10\n",
      "        6907       0.97      0.99      0.98        69\n",
      "        6908       0.00      0.00      0.00         1\n",
      "        6909       0.90      0.82      0.86        44\n",
      "        6910       1.00      0.92      0.96        24\n",
      "        6912       0.96      0.97      0.97       111\n",
      "        6913       0.90      0.97      0.94        37\n",
      "        6914       0.81      0.72      0.76        18\n",
      "        7001       1.00      0.92      0.96        24\n",
      "        7003       0.00      0.00      0.00         2\n",
      "        7006       0.83      0.83      0.83        18\n",
      "        7007       0.98      0.97      0.97        60\n",
      "        7009       0.98      0.98      0.98        54\n",
      "        7010       0.95      0.98      0.96       227\n",
      "        7013       0.99      0.93      0.96       129\n",
      "        7014       1.00      0.94      0.97        18\n",
      "        7015       0.94      1.00      0.97        17\n",
      "        7016       0.94      1.00      0.97        16\n",
      "        7017       0.97      0.83      0.90        36\n",
      "        7018       0.93      0.97      0.95        29\n",
      "        7019       0.99      0.99      0.99       162\n",
      "        7020       0.83      0.94      0.88        32\n",
      "        7117       0.98      0.93      0.95        43\n",
      "        7205       0.92      0.73      0.81        15\n",
      "        7208       0.97      1.00      0.98        61\n",
      "        7210       1.00      1.00      1.00        37\n",
      "        7211       0.88      1.00      0.93        14\n",
      "        7213       0.91      1.00      0.95        10\n",
      "        7214       0.95      0.95      0.95        43\n",
      "        7215       0.92      0.92      0.92        13\n",
      "        7216       0.98      0.95      0.96        58\n",
      "        7217       0.95      0.98      0.96        82\n",
      "        7219       0.97      0.97      0.97       182\n",
      "        7220       0.98      0.91      0.95        58\n",
      "        7221       0.00      0.00      0.00         3\n",
      "        7222       0.99      0.96      0.98       103\n",
      "        7223       1.00      0.92      0.96        13\n",
      "        7225       0.92      1.00      0.96        11\n",
      "        7227       0.86      1.00      0.93        19\n",
      "        7228       0.97      0.97      0.97        71\n",
      "        7229       0.91      1.00      0.95        31\n",
      "        7303       1.00      0.87      0.93        15\n",
      "        7304       0.98      0.97      0.97       238\n",
      "        7306       0.97      0.94      0.95        99\n",
      "        7307       0.98      0.97      0.98       287\n",
      "        7308       0.97      0.91      0.94        68\n",
      "        7309       0.86      0.67      0.75        18\n",
      "        7310       0.91      0.89      0.90        72\n",
      "        7311       1.00      0.69      0.82        13\n",
      "        7312       0.97      0.97      0.97       176\n",
      "        7314       0.96      0.96      0.96        81\n",
      "        7315       0.94      0.96      0.95       128\n",
      "        7316       1.00      1.00      1.00        17\n",
      "        7317       0.88      0.91      0.90        58\n",
      "        7318       0.97      0.99      0.98       702\n",
      "        7319       0.93      0.89      0.91        46\n",
      "        7320       0.97      0.99      0.98       158\n",
      "        7321       0.95      0.96      0.95        73\n",
      "        7322       0.96      0.74      0.84        31\n",
      "        7323       0.94      1.00      0.97       122\n",
      "        7324       0.97      0.95      0.96        37\n",
      "        7325       0.96      0.96      0.96        55\n",
      "        7326       0.96      0.90      0.93       292\n",
      "        7407       1.00      0.86      0.92        14\n",
      "        7409       1.00      0.83      0.91        12\n",
      "        7411       1.00      0.97      0.99        70\n",
      "        7412       0.91      0.95      0.93        22\n",
      "        7415       0.89      0.92      0.91        64\n",
      "        7418       1.00      1.00      1.00        17\n",
      "        7419       0.67      0.75      0.71        24\n",
      "        7507       1.00      1.00      1.00        19\n",
      "        7604       0.96      0.98      0.97        47\n",
      "        7605       1.00      1.00      1.00        13\n",
      "        7606       1.00      0.96      0.98        49\n",
      "        7607       1.00      1.00      1.00        21\n",
      "        7608       0.97      0.97      0.97        31\n",
      "        7609       1.00      0.89      0.94         9\n",
      "        7610       1.00      1.00      1.00        13\n",
      "        7612       0.82      0.88      0.85        26\n",
      "        7615       0.96      0.86      0.91        29\n",
      "        7616       0.95      0.97      0.96       117\n",
      "        7907       1.00      0.97      0.99        36\n",
      "        8003       0.95      0.86      0.90        21\n",
      "        8101       0.96      0.96      0.96        49\n",
      "        8108       1.00      0.89      0.94        18\n",
      "        8111       1.00      0.93      0.96        14\n",
      "        8113       0.86      0.86      0.86        14\n",
      "        8201       0.98      0.98      0.98       131\n",
      "        8202       0.96      0.99      0.98        82\n",
      "        8203       0.95      0.94      0.95        85\n",
      "        8204       1.00      0.98      0.99        57\n",
      "        8205       0.88      0.95      0.91       173\n",
      "        8206       0.89      0.76      0.82        21\n",
      "        8207       0.96      0.97      0.97       354\n",
      "        8208       0.93      0.90      0.92        90\n",
      "        8209       0.67      0.22      0.33         9\n",
      "        8210       1.00      0.92      0.96        12\n",
      "        8211       0.97      0.99      0.98       100\n",
      "        8212       0.96      1.00      0.98        26\n",
      "        8213       1.00      0.94      0.97        31\n",
      "        8214       0.91      0.91      0.91        34\n",
      "        8215       1.00      0.94      0.97        34\n",
      "        8301       0.96      0.98      0.97       174\n",
      "        8302       0.96      0.95      0.95       214\n",
      "        8303       1.00      0.93      0.97        15\n",
      "        8304       0.80      1.00      0.89        12\n",
      "        8305       0.89      0.93      0.91        54\n",
      "        8306       0.90      0.97      0.93        63\n",
      "        8307       0.90      0.96      0.93        48\n",
      "        8308       0.89      0.89      0.89        53\n",
      "        8309       0.90      0.79      0.84        33\n",
      "        8310       1.00      0.79      0.88        19\n",
      "        8311       0.94      0.91      0.92        65\n",
      "        8402       1.00      1.00      1.00        17\n",
      "        8407       1.00      0.99      0.99       159\n",
      "        8408       0.97      1.00      0.99       108\n",
      "        8409       0.84      0.75      0.79        36\n",
      "        8412       0.96      0.95      0.95       168\n",
      "        8413       0.95      0.98      0.96       404\n",
      "        8414       0.97      0.97      0.97       362\n",
      "        8415       1.00      0.94      0.97        33\n",
      "        8416       1.00      0.89      0.94        18\n",
      "        8417       0.97      0.86      0.91        36\n",
      "        8418       0.99      0.97      0.98        90\n",
      "        8419       0.82      0.86      0.84       127\n",
      "        8420       0.95      0.63      0.76        30\n",
      "        8421       0.82      0.92      0.87       211\n",
      "        8422       0.91      0.95      0.93        64\n",
      "        8423       0.95      0.99      0.97       135\n",
      "        8424       0.90      0.93      0.91       221\n",
      "        8425       0.91      0.93      0.92        80\n",
      "        8426       0.98      0.98      0.98        61\n",
      "        8427       0.94      0.97      0.95        62\n",
      "        8428       0.88      0.90      0.89        84\n",
      "        8429       1.00      1.00      1.00        59\n",
      "        8430       0.98      0.93      0.96        58\n",
      "        8431       0.88      0.86      0.87       145\n",
      "        8432       0.97      0.95      0.96        75\n",
      "        8433       0.90      0.93      0.91        28\n",
      "        8438       0.95      0.94      0.94        63\n",
      "        8441       0.97      0.99      0.98        88\n",
      "        8443       0.95      0.99      0.97       106\n",
      "        8444       0.88      0.92      0.90        25\n",
      "        8445       0.87      0.93      0.90        29\n",
      "        8446       1.00      1.00      1.00        29\n",
      "        8447       1.00      1.00      1.00        31\n",
      "        8448       0.90      0.94      0.92        77\n",
      "        8450       1.00      1.00      1.00        12\n",
      "        8451       1.00      0.83      0.91        47\n",
      "        8452       0.95      0.98      0.97       101\n",
      "        8453       0.96      0.96      0.96        26\n",
      "        8454       0.97      1.00      0.98        56\n",
      "        8455       0.97      0.97      0.97        36\n",
      "        8456       0.97      0.98      0.98        65\n",
      "        8457       0.75      0.96      0.84        25\n",
      "        8458       1.00      0.97      0.98        29\n",
      "        8459       0.93      0.86      0.89        73\n",
      "        8460       0.94      0.96      0.95        46\n",
      "        8461       0.93      0.96      0.95        54\n",
      "        8462       0.93      0.93      0.93       102\n",
      "        8463       0.94      1.00      0.97        34\n",
      "        8464       0.98      0.98      0.98        48\n",
      "        8465       0.95      0.90      0.92       115\n",
      "        8466       0.90      0.95      0.93       133\n",
      "        8467       0.99      0.95      0.97       319\n",
      "        8468       0.91      0.89      0.90        45\n",
      "        8470       0.97      0.97      0.97        35\n",
      "        8471       0.99      0.98      0.99       530\n",
      "        8472       1.00      0.91      0.95        11\n",
      "        8473       0.89      0.91      0.90        54\n",
      "        8474       0.94      0.87      0.91       203\n",
      "        8477       0.84      0.95      0.89       160\n",
      "        8479       0.80      0.75      0.77       150\n",
      "        8480       0.92      0.89      0.91        27\n",
      "        8481       0.97      0.97      0.97       513\n",
      "        8482       0.94      0.96      0.95       191\n",
      "        8483       0.94      0.96      0.95       408\n",
      "        8484       0.79      0.93      0.85        54\n",
      "        8485       0.00      0.00      0.00         1\n",
      "        8487       0.86      0.88      0.87        42\n",
      "        8501       1.00      0.98      0.99       312\n",
      "        8502       0.98      0.99      0.99       115\n",
      "        8503       0.78      0.75      0.76        28\n",
      "        8504       0.98      0.98      0.98       791\n",
      "        8505       0.96      0.97      0.97       104\n",
      "        8506       0.98      1.00      0.99       123\n",
      "        8507       1.00      0.97      0.99       114\n",
      "        8508       0.96      1.00      0.98        89\n",
      "        8509       0.96      0.90      0.93        29\n",
      "        8510       0.96      0.96      0.96        52\n",
      "        8511       0.97      0.95      0.96       100\n",
      "        8512       0.98      0.95      0.96       225\n",
      "        8513       0.98      1.00      0.99        56\n",
      "        8514       0.88      0.90      0.89        48\n",
      "        8515       0.95      0.99      0.97       171\n",
      "        8516       0.97      0.97      0.97       379\n",
      "        8517       0.95      0.95      0.95       106\n",
      "        8518       0.98      0.98      0.98        99\n",
      "        8519       0.98      0.99      0.98        94\n",
      "        8521       0.97      0.97      0.97        34\n",
      "        8522       1.00      1.00      1.00         8\n",
      "        8523       1.00      0.95      0.98       144\n",
      "        8524       0.70      0.78      0.74         9\n",
      "        8525       0.98      0.98      0.98       121\n",
      "        8526       1.00      0.73      0.84        11\n",
      "        8527       0.96      0.97      0.96       110\n",
      "        8528       0.97      0.93      0.95       119\n",
      "        8529       0.96      0.95      0.96        82\n",
      "        8531       0.85      0.93      0.89        80\n",
      "        8532       1.00      1.00      1.00        31\n",
      "        8533       1.00      0.96      0.98        75\n",
      "        8534       0.98      0.96      0.97        52\n",
      "        8535       0.79      0.76      0.78        25\n",
      "        8536       0.94      0.95      0.94       384\n",
      "        8537       0.93      0.89      0.91        63\n",
      "        8538       0.82      0.88      0.85        66\n",
      "        8539       0.99      1.00      1.00       329\n",
      "        8540       1.00      0.92      0.96        12\n",
      "        8541       0.96      0.96      0.96       128\n",
      "        8542       1.00      1.00      1.00        17\n",
      "        8543       0.84      0.87      0.85        77\n",
      "        8544       0.98      0.97      0.98       308\n",
      "        8545       0.94      0.93      0.94        72\n",
      "        8546       0.95      1.00      0.98        20\n",
      "        8547       1.00      0.95      0.98        22\n",
      "        8548       0.88      0.93      0.90        15\n",
      "        8606       1.00      1.00      1.00        15\n",
      "        8607       0.71      1.00      0.83         5\n",
      "        8609       0.95      1.00      0.97        19\n",
      "        8701       1.00      0.98      0.99        51\n",
      "        8703       1.00      1.00      1.00        27\n",
      "        8707       0.80      0.86      0.83        14\n",
      "        8708       0.98      0.97      0.97      1291\n",
      "        8711       1.00      0.97      0.99        68\n",
      "        8712       0.94      0.97      0.96        34\n",
      "        8714       0.98      0.97      0.97       316\n",
      "        8715       0.92      1.00      0.96        36\n",
      "        8716       0.97      0.86      0.91        65\n",
      "        8802       0.92      1.00      0.96        12\n",
      "        8806       0.00      0.00      0.00         2\n",
      "        8903       0.92      1.00      0.96        12\n",
      "        8905       1.00      1.00      1.00        17\n",
      "        8907       1.00      1.00      1.00        19\n",
      "        9001       1.00      0.96      0.98        89\n",
      "        9002       0.91      0.98      0.94        41\n",
      "        9003       1.00      1.00      1.00        62\n",
      "        9004       1.00      1.00      1.00        64\n",
      "        9005       1.00      0.97      0.98        30\n",
      "        9008       1.00      1.00      1.00        14\n",
      "        9011       0.92      1.00      0.96        11\n",
      "        9013       0.93      0.93      0.93        27\n",
      "        9014       0.92      0.96      0.94        23\n",
      "        9015       1.00      0.97      0.99        39\n",
      "        9016       0.94      0.84      0.89        19\n",
      "        9017       0.92      0.97      0.94        95\n",
      "        9018       0.94      0.92      0.93       127\n",
      "        9019       0.94      0.91      0.92        80\n",
      "        9020       1.00      0.91      0.95        23\n",
      "        9022       1.00      0.96      0.98        45\n",
      "        9023       0.86      0.76      0.81        25\n",
      "        9024       0.92      0.85      0.88        13\n",
      "        9025       0.91      0.96      0.93       103\n",
      "        9026       0.91      0.95      0.93       166\n",
      "        9027       0.83      0.85      0.84        89\n",
      "        9028       0.94      0.94      0.94        31\n",
      "        9029       0.92      0.89      0.90        80\n",
      "        9030       0.89      0.96      0.92       163\n",
      "        9031       0.86      0.87      0.87       130\n",
      "        9032       0.92      0.80      0.86        89\n",
      "        9033       0.57      0.44      0.50        18\n",
      "        9102       0.96      1.00      0.98        46\n",
      "        9104       1.00      1.00      1.00        12\n",
      "        9105       1.00      0.99      0.99        67\n",
      "        9106       0.85      1.00      0.92        11\n",
      "        9107       1.00      0.92      0.96        12\n",
      "        9112       0.96      0.96      0.96        23\n",
      "        9113       1.00      1.00      1.00        18\n",
      "        9114       1.00      1.00      1.00        19\n",
      "        9202       1.00      1.00      1.00        21\n",
      "        9205       1.00      1.00      1.00        23\n",
      "        9206       0.89      1.00      0.94         8\n",
      "        9207       1.00      0.97      0.98        63\n",
      "        9208       0.96      0.96      0.96        25\n",
      "        9209       1.00      1.00      1.00        10\n",
      "        9304       1.00      1.00      1.00        13\n",
      "        9305       0.88      0.93      0.90        15\n",
      "        9307       1.00      0.89      0.94        19\n",
      "        9401       0.97      0.97      0.97       183\n",
      "        9402       0.91      1.00      0.95        10\n",
      "        9403       0.99      0.98      0.98       511\n",
      "        9404       0.99      0.98      0.98        88\n",
      "        9405       1.00      0.99      0.99       720\n",
      "        9406       1.00      0.96      0.98        45\n",
      "        9503       0.99      0.99      0.99      1739\n",
      "        9504       0.99      0.97      0.98       275\n",
      "        9505       0.96      1.00      0.98        51\n",
      "        9506       0.98      0.99      0.99       300\n",
      "        9507       0.98      1.00      0.99        88\n",
      "        9602       0.94      0.83      0.88        18\n",
      "        9603       0.95      0.98      0.97       211\n",
      "        9604       0.92      1.00      0.96        12\n",
      "        9605       0.77      1.00      0.87        10\n",
      "        9606       0.94      0.98      0.96        50\n",
      "        9607       1.00      0.96      0.98        67\n",
      "        9608       0.95      0.96      0.95       164\n",
      "        9609       0.98      0.97      0.97       124\n",
      "        9610       0.94      0.94      0.94        17\n",
      "        9611       1.00      0.73      0.85        15\n",
      "        9612       1.00      1.00      1.00         8\n",
      "        9613       0.98      0.92      0.95        52\n",
      "        9614       1.00      1.00      1.00        10\n",
      "        9615       1.00      0.94      0.97        34\n",
      "        9616       0.89      0.93      0.91        61\n",
      "        9617       0.92      1.00      0.96        11\n",
      "        9618       0.96      0.96      0.96        23\n",
      "        9619       1.00      1.00      1.00       101\n",
      "        9620       0.99      0.98      0.98        86\n",
      "\n",
      "    accuracy                           0.95     46596\n",
      "   macro avg       0.93      0.92      0.92     46596\n",
      "weighted avg       0.96      0.95      0.95     46596\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "Label_encoder = preprocessing.LabelEncoder()\n",
    "Label_encoder.classes_ = np.load('./cl_classes2610.npy', allow_pickle=True)\n",
    "\n",
    "true = []\n",
    "preds = []\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(test_dataloader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    true += batch[\"labels\"].detach().cpu().numpy().tolist()\n",
    "    preds += predictions.detach().cpu().numpy().tolist()\n",
    "\n",
    "print(classification_report(Label_encoder.inverse_transform(true), Label_encoder.inverse_transform(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2efb846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(text):\n",
    "    model.to(torch.device('cpu'))\n",
    "    inputs = tokenizer(text, truncation = True, max_length=100, padding='max_length', return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "        return Label_encoder.inverse_transform([predicted_class_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a15ea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8712'] ['8712'] ['8712']\n"
     ]
    }
   ],
   "source": [
    "print(predict_class(\"Велосипед\"), predict_class(\"велосипед\"), predict_class(\"Велосипедов\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "532e5aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4014'] ['4014'] ['9608'] ['5607']\n"
     ]
    }
   ],
   "source": [
    "print(predict_class(\"презерватив\"), predict_class(\"презервативов\"), predict_class(\"гандоны\"), predict_class(\"гандон\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
