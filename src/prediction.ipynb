{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a36a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/sbert_large_nlu_ru and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#from transformers import BertTokenizer, BertModel, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')                                                     \n",
    "#model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=658).to(device)\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "model_name = \"sberbank-ai/sbert_large_nlu_ru\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=314).to(device)\n",
    "from sklearn import preprocessing\n",
    "Label_encoder = preprocessing.LabelEncoder()\n",
    "Label_encoder.classes_ = np.load('./classes.npy', allow_pickle=True)\n",
    "model.load_state_dict(torch.load(\"../../best_model3004/pytorch_model.bin\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b11f3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8752dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(text):\n",
    "    model.to(torch.device('cpu'))\n",
    "    inputs = tokenizer(text, truncation = True, max_length=100, padding='max_length', return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "        return Label_encoder.inverse_transform([predicted_class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1770ccb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4011'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class('Автомобильные покрышки резиновые новые')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c609c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(text, qtty=2):\n",
    "    model.to(torch.device('cpu'))\n",
    "    inputs = tokenizer(text, truncation = True, max_length=100, padding='max_length', return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    result = dict()\n",
    "    p = torch.nn.functional.softmax(logits, dim=1)\n",
    "    for i in range(qtty):\n",
    "        a = p.argmax().item()\n",
    "        result[Label_encoder.inverse_transform([a])[0]] = p[0][a].item()\n",
    "        p[0][a] = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7dc850b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0808': 0.8409392833709717, '0805': 0.014152244664728642, '2007': 0.006393834948539734, '6702': 0.005769975483417511, '2106': 0.005318484269082546}\n"
     ]
    }
   ],
   "source": [
    "print(predict_prob('бананы в коробках', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5368983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code:    prob-ty\n",
      "8477:    0.27823\n",
      "3926:    0.13464\n",
      "6406:    0.03673\n",
      "9608:    0.01928\n",
      "3923:    0.01838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict_prob(\"гелевая ручка\", 5)\n",
    "print('Code', 'prob-ty', sep=':    ')\n",
    "[print(i, f\"{preds[i]:.{5}f}\", sep=':    ') for i in preds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6501e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolelog(probs):\n",
    "    print('Code', 'prob-ty', sep=':|    ')\n",
    "    print('_________________')\n",
    "    for each in probs:\n",
    "        print(each, f\"{probs[each]:.{5}f}\", sep=':|    ')\n",
    "        print('_________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d31aa100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code:|    prob-ty\n",
      "_________________\n",
      "4011:|    0.99986\n",
      "_________________\n",
      "8714:|    0.00001\n",
      "_________________\n",
      "8716:|    0.00001\n",
      "_________________\n",
      "4012:|    0.00001\n",
      "_________________\n",
      "8483:|    0.00000\n",
      "_________________\n",
      "8708:|    0.00000\n",
      "_________________\n",
      "4009:|    0.00000\n",
      "_________________\n",
      "8409:|    0.00000\n",
      "_________________\n",
      "4810:|    0.00000\n",
      "_________________\n",
      "8419:|    0.00000\n",
      "_________________\n"
     ]
    }
   ],
   "source": [
    "consolelog(predict_prob(\"шины новые\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df215eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9996eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "136c8190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Temp\\ipykernel_12380\\3618075809.py:3: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n",
      "  set_caching_enabled(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/yeril/.cache/huggingface/datasets/csv/default-35c82bc51b408539/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1999.19it/s]\n",
      "Extracting data files: 100%|████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 249.09it/s]\n",
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/yeril/.cache/huggingface/datasets/csv/default-35c82bc51b408539/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 189.23it/s]\n",
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "from datasets import load_dataset, load_metric, set_caching_enabled\n",
    "set_caching_enabled(False)\n",
    "dataset = load_dataset('csv', data_files={'train': '../../train.csv', 'test': '../../test.csv'})\n",
    "dataset = dataset.map(lambda e: tokenizer(e['description'], truncation = True, max_length=100, padding='max_length'), batched=True)\n",
    "pytorch_style_columns = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']  # \n",
    "dataset = dataset.remove_columns(set(dataset['train'].features.keys()) - set(pytorch_style_columns))\n",
    "dataset.set_format(type='torch', columns=pytorch_style_columns, device='cuda') \n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset['train'], shuffle=True, batch_size=1)\n",
    "test_dataloader = DataLoader(dataset['test'], shuffle=False, batch_size=1)\n",
    "from tqdm import tqdm\n",
    "#from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "#Label_encoder = preprocessing.LabelEncoder()\n",
    "#Label_encoder.classes_ = np.load('./classes.npy', allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01d6054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 17209/17209 [16:17<00:00, 17.61it/s]\n",
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0805       1.00      1.00      1.00         9\n",
      "        0808       1.00      1.00      1.00        16\n",
      "        1704       0.99      0.99      0.99        80\n",
      "        1905       1.00      1.00      1.00        14\n",
      "        2003       1.00      1.00      1.00        15\n",
      "        2007       1.00      1.00      1.00         7\n",
      "        2009       1.00      1.00      1.00         8\n",
      "        2105       1.00      1.00      1.00         8\n",
      "        2106       0.84      0.90      0.87        30\n",
      "        2202       0.60      1.00      0.75         3\n",
      "        2710       1.00      1.00      1.00         8\n",
      "        2918       0.76      0.90      0.83        29\n",
      "        2924       0.71      0.81      0.76        27\n",
      "        2933       0.68      0.75      0.71        55\n",
      "        2934       0.81      0.69      0.75        36\n",
      "        2941       0.95      0.86      0.90        22\n",
      "        3005       1.00      1.00      1.00        14\n",
      "        3206       0.88      0.95      0.91        22\n",
      "        3213       0.86      1.00      0.92         6\n",
      "        3214       0.94      0.89      0.91        18\n",
      "        3304       0.96      0.98      0.97        49\n",
      "        3306       0.90      1.00      0.95        19\n",
      "        3401       0.86      0.92      0.89        13\n",
      "        3402       0.93      0.93      0.93        28\n",
      "        3403       0.85      0.92      0.88        12\n",
      "        3404       1.00      0.85      0.92        26\n",
      "        3407       0.95      1.00      0.98        21\n",
      "        3506       1.00      0.95      0.98        21\n",
      "        3802       1.00      1.00      1.00         8\n",
      "        3808       1.00      1.00      1.00        30\n",
      "        3816       0.82      1.00      0.90        14\n",
      "        3824       0.71      0.62      0.67        16\n",
      "        3906       1.00      0.73      0.84        11\n",
      "        3916       0.94      0.96      0.95        46\n",
      "        3917       0.89      0.95      0.92       114\n",
      "        3918       0.97      1.00      0.99        35\n",
      "        3919       0.88      0.96      0.92        71\n",
      "        3920       0.95      0.96      0.95        90\n",
      "        3921       0.88      0.97      0.92        29\n",
      "        3922       1.00      0.71      0.83         7\n",
      "        3923       0.90      0.92      0.91        61\n",
      "        3924       0.96      0.96      0.96       115\n",
      "        3925       0.96      0.93      0.95        28\n",
      "        3926       0.95      0.93      0.94       456\n",
      "        4008       1.00      0.89      0.94        27\n",
      "        4009       0.95      0.92      0.94        64\n",
      "        4010       0.95      0.97      0.96        58\n",
      "        4011       1.00      1.00      1.00      1989\n",
      "        4012       1.00      0.75      0.86         4\n",
      "        4013       1.00      1.00      1.00        22\n",
      "        4014       1.00      1.00      1.00         9\n",
      "        4016       0.94      0.95      0.95       171\n",
      "        4202       0.98      0.96      0.97       125\n",
      "        4205       0.89      0.80      0.84        10\n",
      "        4409       1.00      1.00      1.00         6\n",
      "        4411       1.00      1.00      1.00        27\n",
      "        4412       1.00      1.00      1.00        11\n",
      "        4418       0.86      1.00      0.92         6\n",
      "        4419       1.00      1.00      1.00         8\n",
      "        4420       0.85      1.00      0.92        11\n",
      "        4421       1.00      0.93      0.97        15\n",
      "        4602       1.00      1.00      1.00         2\n",
      "        4810       1.00      1.00      1.00        26\n",
      "        4811       0.90      1.00      0.95        18\n",
      "        4812       0.86      0.86      0.86         7\n",
      "        4819       0.91      1.00      0.95        42\n",
      "        4820       0.97      0.97      0.97        39\n",
      "        4821       0.83      0.91      0.87        11\n",
      "        4823       0.86      0.70      0.78        27\n",
      "        4910       1.00      1.00      1.00         6\n",
      "        4911       0.97      0.86      0.91        36\n",
      "        5208       1.00      1.00      1.00        21\n",
      "        5401       0.92      0.85      0.88        13\n",
      "        5404       0.75      1.00      0.86         3\n",
      "        5407       0.92      0.85      0.88        13\n",
      "        5508       0.71      1.00      0.83         5\n",
      "        5514       1.00      1.00      1.00        18\n",
      "        5601       1.00      1.00      1.00         5\n",
      "        5602       0.88      0.70      0.78        20\n",
      "        5603       0.94      0.94      0.94        53\n",
      "        5604       0.92      0.92      0.92        12\n",
      "        5607       1.00      0.97      0.98        30\n",
      "        5609       1.00      1.00      1.00         4\n",
      "        5703       0.96      0.96      0.96        28\n",
      "        5704       0.93      1.00      0.96        13\n",
      "        5705       1.00      0.86      0.92         7\n",
      "        5806       0.93      0.93      0.93        14\n",
      "        5808       0.91      0.91      0.91        32\n",
      "        5903       0.80      0.80      0.80        15\n",
      "        5906       0.71      1.00      0.83         5\n",
      "        5907       0.73      0.65      0.69        17\n",
      "        5911       0.83      0.75      0.79        20\n",
      "        6001       1.00      1.00      1.00        10\n",
      "        6115       1.00      1.00      1.00        10\n",
      "        6116       0.88      0.98      0.92        44\n",
      "        6117       0.00      0.00      0.00         1\n",
      "        6203       1.00      0.87      0.93        15\n",
      "        6211       0.88      0.96      0.92        23\n",
      "        6213       0.90      1.00      0.95         9\n",
      "        6214       1.00      0.97      0.98        30\n",
      "        6216       1.00      1.00      1.00         7\n",
      "        6217       0.92      0.92      0.92        13\n",
      "        6302       1.00      1.00      1.00        29\n",
      "        6303       1.00      1.00      1.00         5\n",
      "        6304       0.85      0.92      0.88        24\n",
      "        6305       0.96      1.00      0.98        26\n",
      "        6306       1.00      0.91      0.95        22\n",
      "        6307       0.98      0.90      0.94       104\n",
      "        6402       0.92      0.85      0.88        13\n",
      "        6403       0.95      1.00      0.97        38\n",
      "        6406       1.00      0.97      0.99        38\n",
      "        6505       1.00      1.00      1.00        11\n",
      "        6506       1.00      0.92      0.96        13\n",
      "        6601       0.95      1.00      0.98        20\n",
      "        6603       1.00      1.00      1.00        10\n",
      "        6702       1.00      1.00      1.00        12\n",
      "        6704       1.00      0.89      0.94         9\n",
      "        6802       1.00      0.95      0.97        59\n",
      "        6804       0.91      0.91      0.91        23\n",
      "        6805       0.88      1.00      0.93        14\n",
      "        6812       0.67      0.80      0.73         5\n",
      "        6813       0.70      0.78      0.74        27\n",
      "        6815       0.86      0.92      0.89        13\n",
      "        6910       1.00      0.92      0.96        12\n",
      "        6912       1.00      0.89      0.94        35\n",
      "        6913       0.67      0.31      0.42        13\n",
      "        6914       0.75      1.00      0.86         9\n",
      "        7007       1.00      1.00      1.00        17\n",
      "        7009       0.98      1.00      0.99        53\n",
      "        7010       0.95      0.91      0.93        23\n",
      "        7013       0.95      0.98      0.96        56\n",
      "        7018       0.89      0.89      0.89         9\n",
      "        7019       1.00      0.99      0.99        90\n",
      "        7117       0.92      0.92      0.92        25\n",
      "        7217       0.92      1.00      0.96        11\n",
      "        7219       1.00      1.00      1.00         6\n",
      "        7304       0.96      0.98      0.97        98\n",
      "        7306       1.00      0.95      0.97        19\n",
      "        7307       0.92      0.96      0.94        96\n",
      "        7308       0.96      0.89      0.93        28\n",
      "        7310       0.92      0.92      0.92        26\n",
      "        7312       0.98      0.92      0.95        92\n",
      "        7314       0.96      0.85      0.90        26\n",
      "        7315       0.88      0.98      0.93        86\n",
      "        7317       0.85      1.00      0.92        23\n",
      "        7318       0.96      0.95      0.96       599\n",
      "        7319       1.00      0.96      0.98        24\n",
      "        7320       1.00      0.96      0.98        28\n",
      "        7321       0.84      0.95      0.89        22\n",
      "        7323       0.96      0.98      0.97       100\n",
      "        7324       0.86      1.00      0.92         6\n",
      "        7325       1.00      1.00      1.00        26\n",
      "        7326       0.96      0.88      0.92       183\n",
      "        7411       0.96      0.88      0.92        25\n",
      "        7412       1.00      0.80      0.89         5\n",
      "        7415       0.85      0.85      0.85        13\n",
      "        7419       1.00      0.60      0.75        15\n",
      "        7604       1.00      1.00      1.00        19\n",
      "        7608       1.00      0.67      0.80         6\n",
      "        7615       1.00      0.75      0.86         4\n",
      "        7616       0.96      0.98      0.97        53\n",
      "        8111       1.00      1.00      1.00        12\n",
      "        8201       0.92      1.00      0.96        36\n",
      "        8202       0.94      1.00      0.97        30\n",
      "        8203       0.80      0.87      0.83        23\n",
      "        8204       0.88      1.00      0.94        30\n",
      "        8205       0.91      0.89      0.90        90\n",
      "        8206       0.88      0.88      0.88         8\n",
      "        8207       0.91      0.91      0.91        75\n",
      "        8208       0.82      0.74      0.78        31\n",
      "        8210       1.00      1.00      1.00        12\n",
      "        8211       0.96      0.92      0.94        25\n",
      "        8212       1.00      0.91      0.95        11\n",
      "        8213       1.00      0.93      0.97        15\n",
      "        8214       0.83      0.96      0.89        26\n",
      "        8215       1.00      1.00      1.00        13\n",
      "        8301       0.93      0.93      0.93        75\n",
      "        8302       0.95      0.97      0.96       134\n",
      "        8304       0.92      0.85      0.88        13\n",
      "        8305       1.00      1.00      1.00         5\n",
      "        8307       1.00      1.00      1.00         5\n",
      "        8308       0.94      0.94      0.94        32\n",
      "        8309       1.00      0.57      0.73         7\n",
      "        8311       0.80      1.00      0.89         4\n",
      "        8407       0.98      0.97      0.98        62\n",
      "        8409       0.87      0.80      0.83        49\n",
      "        8412       0.90      0.84      0.87        95\n",
      "        8413       0.98      0.94      0.96       123\n",
      "        8414       0.95      0.95      0.95       103\n",
      "        8415       1.00      0.86      0.92         7\n",
      "        8419       0.77      0.88      0.82        26\n",
      "        8421       0.82      0.90      0.86       124\n",
      "        8422       0.67      1.00      0.80         8\n",
      "        8423       0.84      0.98      0.90        48\n",
      "        8424       0.92      0.85      0.89        41\n",
      "        8425       0.94      1.00      0.97        33\n",
      "        8426       1.00      1.00      1.00        11\n",
      "        8427       1.00      0.75      0.86         4\n",
      "        8428       0.87      0.87      0.87        30\n",
      "        8429       1.00      1.00      1.00        15\n",
      "        8431       0.79      0.84      0.82        58\n",
      "        8443       1.00      1.00      1.00        16\n",
      "        8452       1.00      1.00      1.00        20\n",
      "        8454       1.00      0.73      0.85        15\n",
      "        8456       0.84      1.00      0.91        16\n",
      "        8462       0.75      0.96      0.84        28\n",
      "        8463       0.67      0.80      0.73         5\n",
      "        8464       1.00      1.00      1.00         4\n",
      "        8465       1.00      0.12      0.22         8\n",
      "        8466       0.71      0.77      0.74        39\n",
      "        8467       0.88      0.92      0.90        61\n",
      "        8468       1.00      0.73      0.84        11\n",
      "        8470       1.00      0.75      0.86         8\n",
      "        8471       0.96      0.99      0.97        70\n",
      "        8474       0.80      0.85      0.82        66\n",
      "        8477       0.82      0.91      0.86        34\n",
      "        8479       0.88      0.73      0.80        89\n",
      "        8480       0.75      1.00      0.86         3\n",
      "        8481       0.95      0.93      0.94       210\n",
      "        8482       0.97      0.93      0.95        94\n",
      "        8483       0.87      0.94      0.90       234\n",
      "        8484       0.86      0.92      0.89        26\n",
      "        8487       0.80      0.89      0.84        27\n",
      "        8501       0.97      0.97      0.97        95\n",
      "        8502       1.00      0.98      0.99        48\n",
      "        8503       0.58      0.70      0.64        10\n",
      "        8504       0.95      0.97      0.96       118\n",
      "        8505       0.95      0.86      0.90        49\n",
      "        8506       1.00      0.93      0.96        27\n",
      "        8507       0.90      0.95      0.93        20\n",
      "        8508       0.96      0.88      0.92        25\n",
      "        8509       0.92      0.94      0.93        36\n",
      "        8510       1.00      0.93      0.96        28\n",
      "        8511       0.95      0.90      0.92        39\n",
      "        8512       0.95      0.96      0.95       175\n",
      "        8513       1.00      1.00      1.00        71\n",
      "        8515       0.83      0.83      0.83        12\n",
      "        8516       0.99      0.93      0.96       159\n",
      "        8517       0.92      1.00      0.96        36\n",
      "        8518       0.97      0.86      0.91        35\n",
      "        8519       0.80      0.92      0.86        13\n",
      "        8523       1.00      0.76      0.87        17\n",
      "        8525       0.94      0.97      0.95        32\n",
      "        8526       0.67      0.60      0.63        10\n",
      "        8527       0.89      0.91      0.90        46\n",
      "        8528       1.00      1.00      1.00        11\n",
      "        8529       1.00      1.00      1.00         3\n",
      "        8531       0.94      0.85      0.89        20\n",
      "        8533       0.97      0.91      0.94        32\n",
      "        8534       1.00      0.83      0.91        18\n",
      "        8536       0.95      0.95      0.95       212\n",
      "        8537       0.74      0.87      0.80        23\n",
      "        8538       0.82      0.91      0.86        35\n",
      "        8539       0.98      0.98      0.98       146\n",
      "        8541       1.00      0.86      0.92        21\n",
      "        8543       0.83      0.86      0.84        28\n",
      "        8544       0.98      0.97      0.97       144\n",
      "        8545       1.00      0.92      0.96        12\n",
      "        8547       1.00      0.88      0.94        17\n",
      "        8708       0.95      0.97      0.96      1190\n",
      "        8712       1.00      0.94      0.97        17\n",
      "        8714       0.95      0.96      0.95       184\n",
      "        8715       1.00      1.00      1.00        11\n",
      "        8716       0.84      0.89      0.86        18\n",
      "        8903       1.00      1.00      1.00         5\n",
      "        9004       1.00      1.00      1.00        37\n",
      "        9005       0.97      0.94      0.96        35\n",
      "        9011       1.00      1.00      1.00        13\n",
      "        9013       0.95      0.95      0.95        22\n",
      "        9016       0.81      1.00      0.90        13\n",
      "        9017       1.00      0.95      0.97        37\n",
      "        9018       0.98      0.98      0.98        41\n",
      "        9019       0.85      1.00      0.92        11\n",
      "        9022       1.00      1.00      1.00         4\n",
      "        9023       0.93      0.87      0.90        15\n",
      "        9024       0.82      0.90      0.86        10\n",
      "        9025       0.97      0.93      0.95        41\n",
      "        9026       0.71      0.94      0.81        16\n",
      "        9027       0.71      0.83      0.77        18\n",
      "        9028       1.00      0.88      0.93        16\n",
      "        9029       1.00      0.67      0.80        15\n",
      "        9030       1.00      1.00      1.00         7\n",
      "        9031       0.79      0.77      0.78        39\n",
      "        9032       0.97      0.88      0.92        34\n",
      "        9033       0.92      0.63      0.75        19\n",
      "        9102       1.00      1.00      1.00        14\n",
      "        9105       1.00      1.00      1.00         7\n",
      "        9107       1.00      0.77      0.87        13\n",
      "        9113       1.00      1.00      1.00        11\n",
      "        9207       0.97      1.00      0.98        31\n",
      "        9304       1.00      0.90      0.95        10\n",
      "        9401       0.98      0.99      0.98        85\n",
      "        9403       0.99      0.97      0.98       139\n",
      "        9404       1.00      0.88      0.94        17\n",
      "        9405       1.00      0.99      0.99       415\n",
      "        9503       0.99      0.99      0.99      1726\n",
      "        9504       1.00      0.98      0.99       163\n",
      "        9505       1.00      0.95      0.98        43\n",
      "        9506       0.98      0.97      0.98       222\n",
      "        9507       1.00      1.00      1.00        31\n",
      "        9602       1.00      0.92      0.96        13\n",
      "        9603       0.97      0.95      0.96       144\n",
      "        9604       1.00      1.00      1.00         5\n",
      "        9605       0.73      0.89      0.80         9\n",
      "        9606       1.00      0.93      0.96        14\n",
      "        9607       1.00      1.00      1.00        28\n",
      "        9608       0.94      0.93      0.93        54\n",
      "        9609       0.89      0.97      0.93        40\n",
      "        9610       1.00      1.00      1.00        13\n",
      "        9613       0.93      1.00      0.97        14\n",
      "        9615       0.89      0.89      0.89        18\n",
      "        9616       1.00      1.00      1.00        21\n",
      "        9617       0.96      1.00      0.98        22\n",
      "        9620       0.97      0.88      0.92        33\n",
      "\n",
      "    accuracy                           0.95     17209\n",
      "   macro avg       0.93      0.92      0.92     17209\n",
      "weighted avg       0.95      0.95      0.95     17209\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeril\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "true = []\n",
    "preds = []\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(test_dataloader):\n",
    "    batch = {k: v.to(torch.device('cuda')) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    true += batch[\"labels\"].detach().cpu().numpy().tolist()\n",
    "    preds += predictions.detach().cpu().numpy().tolist()\n",
    "\n",
    "print(classification_report(Label_encoder.inverse_transform(true), Label_encoder.inverse_transform(preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
