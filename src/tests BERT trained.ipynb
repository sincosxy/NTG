{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b21c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b13b128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50a72bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')                                                     \n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017370cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/sbert_large_nlu_ru and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "model_name = \"sberbank-ai/sbert_large_nlu_ru\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=314)\n",
    "#–ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–∑–∂–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e513b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load best model\n",
    "model.load_state_dict(torch.load(\"../../best_model3004/pytorch_model.bin\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ceaf084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=314, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0790984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../draft.csv\")\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df['id'] = pd.Series(df['id']).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c972be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_id = []\n",
    "for id in df['id']:\n",
    "    if len(id) == 10:\n",
    "        g_id.append(str(id)[:4])\n",
    "    else:\n",
    "        g_id.append('0' + str(id)[:3])\n",
    "df['g_id'] = g_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75809624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g_id\n",
       "4011    9837\n",
       "9503    8636\n",
       "8708    6086\n",
       "7318    2991\n",
       "3926    2246\n",
       "        ... \n",
       "8427      22\n",
       "7615      21\n",
       "5404      21\n",
       "5609      21\n",
       "4910      21\n",
       "Length: 314, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts('g_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "50d3d28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>g_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>805102000</td>\n",
       "      <td>–ø–ª–æ–¥—ã —Ü–∏—Ç—Ä—É—Å–æ–≤—ã—Ö –∫—É–ª—å—Ç—É—Ä –∞–ø–µ–ª—å—Å–∏–Ω—ã —Å–≤–µ–∂–∏–µ</td>\n",
       "      <td>0805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>805210000</td>\n",
       "      <td>–ø–ª–æ–¥—ã —Ü–∏—Ç—Ä—É—Å–æ–≤—ã—Ö –∫—É–ª—å—Ç—É—Ä –º–∞–Ω–¥–∞—Ä–∏–Ω—ã —Å–≤–µ–∂–∏–µ</td>\n",
       "      <td>0805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>805290000</td>\n",
       "      <td>–ø—Ä–æ–±—ã –∏ –æ–±—Ä–∞–∑—Ü—ã –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Ä–∞–±–æ—Ç –ø–æ –æ—Ü–µ–Ω–∫–µ...</td>\n",
       "      <td>0805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>805501000</td>\n",
       "      <td>–ø–ª–æ–¥—ã —Ü–∏—Ç—Ä—É—Å–æ–≤—ã—Ö –∫—É–ª—å—Ç—É—Ä –ª–∏–º–æ–Ω—ã —Å–≤–µ–∂–∏–µ</td>\n",
       "      <td>0805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>808108002</td>\n",
       "      <td>—è–±–ª–æ–∫–∏ —Å–≤–µ–∂–∏–µ  –≤ –∫–æ—Ä–æ–±–∫–∞—Ö  —Å–æ—Ä—Ç  royal gala  –≤...</td>\n",
       "      <td>0808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87483</th>\n",
       "      <td>1704907500</td>\n",
       "      <td>—Å–ª–∏–≤–æ—á–Ω—ã–µ –∫–æ–Ω—Ñ–µ—Ç—ã  –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –∫–∞–∫–∞–æ  —Ç–º sto...</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87484</th>\n",
       "      <td>1704907500</td>\n",
       "      <td>–ø–∏—â–µ–≤–æ–π –Ω–∞–ø–æ–ª–Ω–∏—Ç–µ–ª—å –¥–ª—è –ø—Ä–æ–º –ø—Ä–æ–∏–∑–≤ –≤–∞ –ø–∏—â–µ–≤–æ–π...</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87485</th>\n",
       "      <td>1704907500</td>\n",
       "      <td>—Ç–æ—Ñ—Ñ–∏ –∫–∞—Ä–∞–º–µ–ª–∏ –∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Å–ª–∞–¥–æ—Å—Ç–∏ –Ω–µ —Å–æ–¥–µ—Ä...</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87486</th>\n",
       "      <td>1704907500</td>\n",
       "      <td>–∫–æ–Ω–¥–∏—Ç–µ—Ä—Å–∫–∏–µ –∏–∑–¥–µ–ª–∏—è –∏–∑ —Å–∞—Ö–∞—Ä–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –∫...</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87487</th>\n",
       "      <td>1704907500</td>\n",
       "      <td>–∫–æ–Ω–¥–∏—Ç–µ—Ä—Å–∫–∏–µ –∏–∑–¥–µ–ª–∏—è –∏–∑ —Å–∞—Ö–∞—Ä–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂a—â–∏–µ –∫...</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86042 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                              label  g_id\n",
       "6       805102000        –ø–ª–æ–¥—ã —Ü–∏—Ç—Ä—É—Å–æ–≤—ã—Ö –∫—É–ª—å—Ç—É—Ä –∞–ø–µ–ª—å—Å–∏–Ω—ã —Å–≤–µ–∂–∏–µ    0805\n",
       "7       805210000        –ø–ª–æ–¥—ã —Ü–∏—Ç—Ä—É—Å–æ–≤—ã—Ö –∫—É–ª—å—Ç—É—Ä –º–∞–Ω–¥–∞—Ä–∏–Ω—ã —Å–≤–µ–∂–∏–µ    0805\n",
       "8       805290000  –ø—Ä–æ–±—ã –∏ –æ–±—Ä–∞–∑—Ü—ã –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Ä–∞–±–æ—Ç –ø–æ –æ—Ü–µ–Ω–∫–µ...  0805\n",
       "9       805501000           –ø–ª–æ–¥—ã —Ü–∏—Ç—Ä—É—Å–æ–≤—ã—Ö –∫—É–ª—å—Ç—É—Ä –ª–∏–º–æ–Ω—ã —Å–≤–µ–∂–∏–µ    0805\n",
       "10      808108002  —è–±–ª–æ–∫–∏ —Å–≤–µ–∂–∏–µ  –≤ –∫–æ—Ä–æ–±–∫–∞—Ö  —Å–æ—Ä—Ç  royal gala  –≤...  0808\n",
       "...           ...                                                ...   ...\n",
       "87483  1704907500  —Å–ª–∏–≤–æ—á–Ω—ã–µ –∫–æ–Ω—Ñ–µ—Ç—ã  –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –∫–∞–∫–∞–æ  —Ç–º sto...  1704\n",
       "87484  1704907500  –ø–∏—â–µ–≤–æ–π –Ω–∞–ø–æ–ª–Ω–∏—Ç–µ–ª—å –¥–ª—è –ø—Ä–æ–º –ø—Ä–æ–∏–∑–≤ –≤–∞ –ø–∏—â–µ–≤–æ–π...  1704\n",
       "87485  1704907500  —Ç–æ—Ñ—Ñ–∏ –∫–∞—Ä–∞–º–µ–ª–∏ –∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Å–ª–∞–¥–æ—Å—Ç–∏ –Ω–µ —Å–æ–¥–µ—Ä...  1704\n",
       "87486  1704907500  –∫–æ–Ω–¥–∏—Ç–µ—Ä—Å–∫–∏–µ –∏–∑–¥–µ–ª–∏—è –∏–∑ —Å–∞—Ö–∞—Ä–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –∫...  1704\n",
       "87487  1704907500  –∫–æ–Ω–¥–∏—Ç–µ—Ä—Å–∫–∏–µ –∏–∑–¥–µ–ª–∏—è –∏–∑ —Å–∞—Ö–∞—Ä–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂a—â–∏–µ –∫...  1704\n",
       "\n",
       "[86042 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_req = 20\n",
    "filtered_classes = df.g_id.value_counts()[df.g_id.value_counts() > min_req].index.values\n",
    "\n",
    "df = df[df.g_id.apply(lambda x: True if x in filtered_classes else False)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "94b596a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.value_counts('id')\n",
    "df_count = pd.DataFrame(data={'id': counts.index, 'count': counts.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1d1e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.to_csv('../../tnveds_sort.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3d05d700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>—Ñ–∏—Ç–∏–Ω–≥–∏ –¥–ª—è —Ç—Ä—É–± –∏–ª–∏ —Ç—Ä—É–±–æ–∫ –∏–∑ –∫–æ—Ä—Ä–æ–∑–∏–Ω–Ω–æ—Å—Ç–æ–π–∫...</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –¥–µ–º–æ–Ω—Ç–∞–∂–∞ –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –∫—Ä—ã—à–∫–∏ –∫...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–±—É–º–∞–≥–∞ –∏–∑ —Ü–µ–ª–ª—é–ª–æ–∑–Ω—ã—Ö –≤–æ–ª–æ–∫–æ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6600005819 —Ä–µ–≥—É–ª—è—Ç–æ—Ä –≤—ã—Å–æ—Ç—ã —Ä–µ–º–Ω—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏...</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>—É–ø–∞–∫–æ–≤–∫–∞ —Å—Ç–µ–∫–ª—è–Ω–Ω–∞—è –¥–ª—è –ø–∞—Ä—Ñ—é–º–µ—Ä–Ω–æ –∫–æ—Å–º–µ—Ç–∏—á–µ—Å–∫...</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86037</th>\n",
       "      <td>–∏–≥—Ä—É—à–∫–∏ –¥–µ—Ç—Å–∫–∏–µ  –∏–º–µ—é—â–∏–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –¥–≤–∏–≥–∞—Ç–µ–ª—å ...</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86038</th>\n",
       "      <td>8891428999 rear bumper body upr –≤–µ—Ä—Ö–Ω–∏–π —ç–ª–µ–º–µ–Ω...</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86039</th>\n",
       "      <td>—Å–≤–µ—Ç–∏–ª—å–Ω–∏–∫–∏ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–µ  –ø–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è —á–∞—Å—Ç–∏—á...</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86040</th>\n",
       "      <td>6666056178 compl a side structure lh –ª–µ–≤–∞—è –±–æ–∫...</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86041</th>\n",
       "      <td>–∏–≥—Ä—É—à–∫–∏ –¥–µ—Ç—Å–∫–∏–µ  –∏–≥—Ä—É—à–µ—á–Ω–æ–µ –æ—Ä—É–∂–∏–µ  —Ä–∞–∑–ª–∏—á–Ω–æ–≥–æ...</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86042 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description  labels\n",
       "0      —Ñ–∏—Ç–∏–Ω–≥–∏ –¥–ª—è —Ç—Ä—É–± –∏–ª–∏ —Ç—Ä—É–±–æ–∫ –∏–∑ –∫–æ—Ä—Ä–æ–∑–∏–Ω–Ω–æ—Å—Ç–æ–π–∫...     138\n",
       "1      –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –¥–µ–º–æ–Ω—Ç–∞–∂–∞ –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –∫—Ä—ã—à–∫–∏ –∫...      43\n",
       "2      –±—É–º–∞–≥–∞ –∏–∑ —Ü–µ–ª–ª—é–ª–æ–∑–Ω—ã—Ö –≤–æ–ª–æ–∫–æ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏...      63\n",
       "3      6600005819 —Ä–µ–≥—É–ª—è—Ç–æ—Ä –≤—ã—Å–æ—Ç—ã —Ä–µ–º–Ω—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏...     177\n",
       "4      —É–ø–∞–∫–æ–≤–∫–∞ —Å—Ç–µ–∫–ª—è–Ω–Ω–∞—è –¥–ª—è –ø–∞—Ä—Ñ—é–º–µ—Ä–Ω–æ –∫–æ—Å–º–µ—Ç–∏—á–µ—Å–∫...     129\n",
       "...                                                  ...     ...\n",
       "86037  –∏–≥—Ä—É—à–∫–∏ –¥–µ—Ç—Å–∫–∏–µ  –∏–º–µ—é—â–∏–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –¥–≤–∏–≥–∞—Ç–µ–ª—å ...     295\n",
       "86038  8891428999 rear bumper body upr –≤–µ—Ä—Ö–Ω–∏–π —ç–ª–µ–º–µ–Ω...     259\n",
       "86039  —Å–≤–µ—Ç–∏–ª—å–Ω–∏–∫–∏ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–µ  –ø–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è —á–∞—Å—Ç–∏—á...     294\n",
       "86040  6666056178 compl a side structure lh –ª–µ–≤–∞—è –±–æ–∫...     259\n",
       "86041  –∏–≥—Ä—É—à–∫–∏ –¥–µ—Ç—Å–∫–∏–µ  –∏–≥—Ä—É—à–µ—á–Ω–æ–µ –æ—Ä—É–∂–∏–µ  —Ä–∞–∑–ª–∏—á–Ω–æ–≥–æ...     295\n",
       "\n",
       "[86042 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b6121bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['label'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "070c4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].apply(lambda x: ' '.join(x.split()[:400]) if len(x.split())>400 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b708368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = df['label'].apply(lambda x: len(x.split())).hist(figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "16849258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9755         –ø–∞—Ç—Ä—É–±–æ–∫\n",
       "10006           —Ö–æ–º—É—Ç\n",
       "15540          —Å—Ç—É–ª—å—è\n",
       "19710         –¥–æ–º–∫—Ä–∞—Ç\n",
       "20438        —Ä–∞—Å—á—ë—Å–∫–∞\n",
       "             ...     \n",
       "86179            –≤–æ—Å–∫\n",
       "86517         —Ço–≤–∞—Ä—ã \n",
       "86655    –∞h—Ç–∏–±–∏–æ—Ç–∏–∫–∏ \n",
       "86931     –¥–∏–ø–∏—Ä–∏–¥a–º–æ–ª\n",
       "87003     —ç—Ç–æ—Ä–∏–∫–æ–∫—Å–∏–±\n",
       "Name: label, Length: 92, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'][df['label'].apply(lambda x: True if len(x.split()) == 1 else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "783679d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "Label_encoder = preprocessing.LabelEncoder()\n",
    "Label_encoder.fit(df['g_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a3ffe27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['g_id'] = Label_encoder.fit_transform(df['g_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "60940c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b9a1fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['description', 'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "866eb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../label_enc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "156dcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./classes.npy', Label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c422c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "train = df[:int(len(df)*0.8)]\n",
    "test =  df[int(len(df)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d98f5d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314\n"
     ]
    }
   ],
   "source": [
    "print(len(test['labels'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6ad8aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../../train.csv', index=False)\n",
    "test.to_csv('../../test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85b090f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=658, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a6fe6090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0925a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f1615b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/sincosxy/.cache/huggingface/datasets/csv/default-47682a937c9addd8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a78c3a8d60e4c8badc5fe09d4993933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "dataset = load_dataset('csv', data_files={'train': '../../train.csv', 'test': '../../test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dca28e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sincosxy/.cache/huggingface/datasets/csv/default-47682a937c9addd8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-32e301bb9fe264f0.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17209 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda e: tokenizer(e['description'], truncation = True, max_length=100, padding='max_length'), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "715c8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_style_columns = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']  # \n",
    "dataset = dataset.remove_columns(set(dataset['train'].features.keys()) - set(pytorch_style_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f1090b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask', 'input_ids', 'labels', 'token_type_ids'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset['train'].features.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e490d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(type='torch', columns=pytorch_style_columns, device='cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3e6cb3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (68833, 4), 'test': (17209, 4)}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3112cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset['train'], shuffle=True, batch_size=8)\n",
    "test_dataloader = DataLoader(dataset['test'], shuffle=False, batch_size=8)\n",
    "#–µ—Å–ª–∏ –µ—Å—Ç—å –≤–∏–¥–µ–æ–ø–∞–º—è—Ç—å –æ–∫–æ–ª–æ 10–≥–±, –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e031d97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8605"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "799638a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sincosxy/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-6)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"constant_with_warmup\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=2000,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55f22b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e98be2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8928b5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=314, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next(iter(train_dataloader))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5a25f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "849bd9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- \n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8605/8605 [39:31<00:00,  3.63it/s]\n",
      "/tmp/ipykernel_3371/2632351315.py:25: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  f1 = load_metric('f1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [99.99%]: 1.752\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2152/2152 [02:47<00:00, 12.85it/s]\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.7996978325294903\n",
      "Test precision: 0.7550712276549277\n",
      "Test recall: 0.7996978325294903\n",
      "Test f1: 0.7565272150455988 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8605/8605 [39:36<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [99.99%]: 1.111\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2152/2152 [02:47<00:00, 12.84it/s]\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.8681503864257075\n",
      "Test precision: 0.8459054347633016\n",
      "Test recall: 0.8681503864257075\n",
      "Test f1: 0.8455024629346195 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8605/8605 [39:33<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [99.99%]: 0.742\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2152/2152 [02:47<00:00, 12.83it/s]\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9027253181474809\n",
      "Test precision: 0.8931912224935824\n",
      "Test recall: 0.9027253181474809\n",
      "Test f1: 0.890272293256992 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8605/8605 [39:35<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [99.99%]: 0.508\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2152/2152 [02:47<00:00, 12.85it/s]\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9209134755070022\n",
      "Test precision: 0.9149767300881549\n",
      "Test recall: 0.9209134755070022\n",
      "Test f1: 0.9131829588454822 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8605/8605 [39:35<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [99.99%]: 0.357\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2152/2152 [02:47<00:00, 12.85it/s]\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9348596664535999\n",
      "Test precision: 0.9317281053634626\n",
      "Test recall: 0.9348596664535999\n",
      "Test f1: 0.9302193516209848 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8605/8605 [39:34<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [99.99%]: 0.257\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2152/2152 [02:47<00:00, 12.85it/s]\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9422395258295078\n",
      "Test precision: 0.9428444020839998\n",
      "Test recall: 0.9422395258295078\n",
      "Test f1: 0.9395694212122289 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8605/8605 [39:34<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [99.99%]: 0.190\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2152/2152 [02:47<00:00, 12.85it/s]\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9446220001162182\n",
      "Test precision: 0.946227708983164\n",
      "Test recall: 0.9446220001162182\n",
      "Test f1: 0.9436794408198166 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8605/8605 [39:35<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [99.99%]: 0.144\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2152/2152 [02:47<00:00, 12.86it/s]\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9479923295949794\n",
      "Test precision: 0.9495223308880203\n",
      "Test recall: 0.9479923295949794\n",
      "Test f1: 0.947315335451981 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8605/8605 [39:34<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [99.99%]: 0.113\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2152/2152 [02:47<00:00, 12.85it/s]\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9490964030449184\n",
      "Test precision: 0.9502615137995498\n",
      "Test recall: 0.9490964030449184\n",
      "Test f1: 0.948790640907274 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8605/8605 [39:34<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [99.99%]: 0.091\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2152/2152 [02:47<00:00, 12.86it/s]\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9500842582369691\n",
      "Test precision: 0.9518276316398866\n",
      "Test recall: 0.9500842582369691\n",
      "Test f1: 0.949821705481887 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8605/8605 [39:34<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss [99.99%]: 0.074\n",
      "\n",
      "validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2152/2152 [02:47<00:00, 12.85it/s]\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted summary:\n",
      "Test acc: 0.9493869486896391\n",
      "Test precision: 0.9514084970219272\n",
      "Test recall: 0.9493869486896391\n",
      "Test f1: 0.9493429811746151 \n",
      "\n",
      "---------------------------------------- \n",
      "epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 5265/8605 [24:13<15:21,  3.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3371/2632351315.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#f i%int(len(train_dataloader)*show_train_loss_every_num_epoch)==int(len(train_dataloader)*show_train_loss_every_num_epoch)-1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_f1 = 0.\n",
    "show_train_loss_every_num_epoch = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(40*'-', '\\nepoch', epoch+1)\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        model.train()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        #f i%int(len(train_dataloader)*show_train_loss_every_num_epoch)==int(len(train_dataloader)*show_train_loss_every_num_epoch)-1:\n",
    "    print(f'train loss [{i*100/len(train_dataloader):.2f}%]: {np.array(losses).mean():.3f}')\n",
    "    losses = []\n",
    "    print('\\nvalidating')\n",
    "\n",
    "    f1 = load_metric('f1')\n",
    "    acc = load_metric('accuracy')\n",
    "    precision = load_metric('precision')\n",
    "    recall = load_metric('recall')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "                \n",
    "        for batch in tqdm(test_dataloader):\n",
    "        ##for batch in test_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            f1.add_batch(predictions=predictions, references=batch[\"labels\"])    \n",
    "            acc.add_batch(predictions=predictions, references=batch[\"labels\"])    \n",
    "            precision.add_batch(predictions=predictions, references=batch[\"labels\"])    \n",
    "            recall.add_batch(predictions=predictions, references=batch[\"labels\"])    \n",
    "\n",
    "        print('weighted summary:')\n",
    "        print('Test acc:', acc.compute()['accuracy'])\n",
    "        print('Test precision:', precision.compute(average = 'weighted')['precision'])\n",
    "        print('Test recall:', recall.compute(average = 'weighted')['recall'])\n",
    "        f1_weighted = f1.compute(average = 'weighted')['f1']\n",
    "        print('Test f1:', f1_weighted, '\\n')\n",
    "\n",
    "        if f1_weighted > best_f1:\n",
    "            best_f1 = f1_weighted\n",
    "            model.save_pretrained(\"../../best_model3004\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8404d0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2152/2152 [02:45<00:00, 13.01it/s]\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0805       0.00      0.00      0.00         9\n",
      "        0808       1.00      0.38      0.55        16\n",
      "        1704       0.38      1.00      0.56        80\n",
      "        1905       0.00      0.00      0.00        14\n",
      "        2003       0.88      1.00      0.94        15\n",
      "        2007       0.00      0.00      0.00         7\n",
      "        2009       0.00      0.00      0.00         8\n",
      "        2105       0.00      0.00      0.00         8\n",
      "        2106       1.00      0.17      0.29        30\n",
      "        2202       0.00      0.00      0.00         3\n",
      "        2710       0.00      0.00      0.00         8\n",
      "        2918       0.00      0.00      0.00        29\n",
      "        2924       0.00      0.00      0.00        27\n",
      "        2933       0.19      0.76      0.31        55\n",
      "        2934       0.00      0.00      0.00        36\n",
      "        2941       0.00      0.00      0.00        22\n",
      "        3005       1.00      1.00      1.00        14\n",
      "        3206       1.00      0.05      0.09        22\n",
      "        3213       0.00      0.00      0.00         6\n",
      "        3214       0.00      0.00      0.00        18\n",
      "        3304       0.73      0.98      0.83        49\n",
      "        3306       0.00      0.00      0.00        19\n",
      "        3401       0.00      0.00      0.00        13\n",
      "        3402       0.38      0.29      0.33        28\n",
      "        3403       0.00      0.00      0.00        12\n",
      "        3404       0.00      0.00      0.00        26\n",
      "        3407       1.00      0.52      0.69        21\n",
      "        3506       0.00      0.00      0.00        21\n",
      "        3802       0.00      0.00      0.00         8\n",
      "        3808       0.72      0.87      0.79        30\n",
      "        3816       0.00      0.00      0.00        14\n",
      "        3824       0.00      0.00      0.00        16\n",
      "        3906       0.00      0.00      0.00        11\n",
      "        3916       0.00      0.00      0.00        46\n",
      "        3917       0.67      0.07      0.13       114\n",
      "        3918       1.00      0.57      0.73        35\n",
      "        3919       0.31      0.55      0.40        71\n",
      "        3920       0.23      0.98      0.37        90\n",
      "        3921       0.00      0.00      0.00        29\n",
      "        3922       0.00      0.00      0.00         7\n",
      "        3923       0.79      0.25      0.38        61\n",
      "        3924       0.65      0.86      0.74       115\n",
      "        3925       1.00      0.36      0.53        28\n",
      "        3926       0.54      0.92      0.68       456\n",
      "        4008       0.00      0.00      0.00        27\n",
      "        4009       1.00      0.06      0.12        64\n",
      "        4010       1.00      0.21      0.34        58\n",
      "        4011       0.97      1.00      0.98      1989\n",
      "        4012       0.00      0.00      0.00         4\n",
      "        4013       1.00      0.64      0.78        22\n",
      "        4014       0.00      0.00      0.00         9\n",
      "        4016       0.55      0.79      0.65       171\n",
      "        4202       0.35      0.98      0.51       125\n",
      "        4205       0.00      0.00      0.00        10\n",
      "        4409       0.00      0.00      0.00         6\n",
      "        4411       0.95      0.74      0.83        27\n",
      "        4412       0.00      0.00      0.00        11\n",
      "        4418       0.00      0.00      0.00         6\n",
      "        4419       0.00      0.00      0.00         8\n",
      "        4420       0.00      0.00      0.00        11\n",
      "        4421       0.00      0.00      0.00        15\n",
      "        4602       0.00      0.00      0.00         2\n",
      "        4810       0.00      0.00      0.00        26\n",
      "        4811       1.00      0.06      0.11        18\n",
      "        4812       0.00      0.00      0.00         7\n",
      "        4819       0.54      0.86      0.66        42\n",
      "        4820       1.00      0.08      0.14        39\n",
      "        4821       0.00      0.00      0.00        11\n",
      "        4823       0.00      0.00      0.00        27\n",
      "        4910       1.00      0.33      0.50         6\n",
      "        4911       1.00      0.39      0.56        36\n",
      "        5208       0.54      0.33      0.41        21\n",
      "        5401       0.00      0.00      0.00        13\n",
      "        5404       0.00      0.00      0.00         3\n",
      "        5407       0.22      0.62      0.33        13\n",
      "        5508       0.00      0.00      0.00         5\n",
      "        5514       0.00      0.00      0.00        18\n",
      "        5601       0.00      0.00      0.00         5\n",
      "        5602       0.00      0.00      0.00        20\n",
      "        5603       0.54      0.55      0.54        53\n",
      "        5604       0.00      0.00      0.00        12\n",
      "        5607       0.00      0.00      0.00        30\n",
      "        5609       0.00      0.00      0.00         4\n",
      "        5703       0.00      0.00      0.00        28\n",
      "        5704       0.00      0.00      0.00        13\n",
      "        5705       0.00      0.00      0.00         7\n",
      "        5806       0.00      0.00      0.00        14\n",
      "        5808       0.00      0.00      0.00        32\n",
      "        5903       0.00      0.00      0.00        15\n",
      "        5906       0.00      0.00      0.00         5\n",
      "        5907       0.00      0.00      0.00        17\n",
      "        5911       0.00      0.00      0.00        20\n",
      "        6001       0.00      0.00      0.00        10\n",
      "        6115       0.00      0.00      0.00        10\n",
      "        6116       0.94      0.34      0.50        44\n",
      "        6117       0.00      0.00      0.00         1\n",
      "        6203       0.00      0.00      0.00        15\n",
      "        6211       0.00      0.00      0.00        23\n",
      "        6213       0.00      0.00      0.00         9\n",
      "        6214       0.00      0.00      0.00        30\n",
      "        6216       0.00      0.00      0.00         7\n",
      "        6217       0.00      0.00      0.00        13\n",
      "        6302       1.00      0.48      0.65        29\n",
      "        6303       0.00      0.00      0.00         5\n",
      "        6304       0.00      0.00      0.00        24\n",
      "        6305       0.00      0.00      0.00        26\n",
      "        6306       0.00      0.00      0.00        22\n",
      "        6307       0.70      0.66      0.68       104\n",
      "        6402       0.00      0.00      0.00        13\n",
      "        6403       0.75      1.00      0.85        38\n",
      "        6406       0.89      0.89      0.89        38\n",
      "        6505       0.00      0.00      0.00        11\n",
      "        6506       0.00      0.00      0.00        13\n",
      "        6601       0.94      0.75      0.83        20\n",
      "        6603       0.00      0.00      0.00        10\n",
      "        6702       1.00      0.67      0.80        12\n",
      "        6704       0.00      0.00      0.00         9\n",
      "        6802       0.84      0.86      0.85        59\n",
      "        6804       0.00      0.00      0.00        23\n",
      "        6805       0.00      0.00      0.00        14\n",
      "        6812       0.00      0.00      0.00         5\n",
      "        6813       0.00      0.00      0.00        27\n",
      "        6815       0.00      0.00      0.00        13\n",
      "        6910       0.00      0.00      0.00        12\n",
      "        6912       0.00      0.00      0.00        35\n",
      "        6913       0.00      0.00      0.00        13\n",
      "        6914       0.00      0.00      0.00         9\n",
      "        7007       0.00      0.00      0.00        17\n",
      "        7009       1.00      0.96      0.98        53\n",
      "        7010       0.00      0.00      0.00        23\n",
      "        7013       0.41      0.95      0.58        56\n",
      "        7018       0.00      0.00      0.00         9\n",
      "        7019       0.69      0.97      0.80        90\n",
      "        7117       0.00      0.00      0.00        25\n",
      "        7217       0.00      0.00      0.00        11\n",
      "        7219       0.00      0.00      0.00         6\n",
      "        7304       0.35      0.96      0.51        98\n",
      "        7306       0.00      0.00      0.00        19\n",
      "        7307       0.71      0.89      0.79        96\n",
      "        7308       0.00      0.00      0.00        28\n",
      "        7310       1.00      0.04      0.07        26\n",
      "        7312       0.60      0.95      0.73        92\n",
      "        7314       0.00      0.00      0.00        26\n",
      "        7315       0.92      0.93      0.92        86\n",
      "        7317       0.00      0.00      0.00        23\n",
      "        7318       0.74      0.97      0.84       599\n",
      "        7319       0.00      0.00      0.00        24\n",
      "        7320       0.00      0.00      0.00        28\n",
      "        7321       0.00      0.00      0.00        22\n",
      "        7323       0.65      0.75      0.69       100\n",
      "        7324       0.00      0.00      0.00         6\n",
      "        7325       0.00      0.00      0.00        26\n",
      "        7326       0.38      0.62      0.47       183\n",
      "        7411       0.00      0.00      0.00        25\n",
      "        7412       0.00      0.00      0.00         5\n",
      "        7415       0.00      0.00      0.00        13\n",
      "        7419       0.00      0.00      0.00        15\n",
      "        7604       0.00      0.00      0.00        19\n",
      "        7608       0.00      0.00      0.00         6\n",
      "        7615       0.00      0.00      0.00         4\n",
      "        7616       0.00      0.00      0.00        53\n",
      "        8111       0.00      0.00      0.00        12\n",
      "        8201       0.00      0.00      0.00        36\n",
      "        8202       0.00      0.00      0.00        30\n",
      "        8203       0.00      0.00      0.00        23\n",
      "        8204       1.00      0.67      0.80        30\n",
      "        8205       0.37      0.71      0.49        90\n",
      "        8206       0.00      0.00      0.00         8\n",
      "        8207       0.24      0.84      0.38        75\n",
      "        8208       0.74      0.65      0.69        31\n",
      "        8210       0.00      0.00      0.00        12\n",
      "        8211       1.00      0.32      0.48        25\n",
      "        8212       1.00      0.64      0.78        11\n",
      "        8213       1.00      0.33      0.50        15\n",
      "        8214       0.00      0.00      0.00        26\n",
      "        8215       0.00      0.00      0.00        13\n",
      "        8301       0.97      0.51      0.67        75\n",
      "        8302       0.56      0.81      0.67       134\n",
      "        8304       0.00      0.00      0.00        13\n",
      "        8305       0.00      0.00      0.00         5\n",
      "        8307       0.00      0.00      0.00         5\n",
      "        8308       0.00      0.00      0.00        32\n",
      "        8309       0.00      0.00      0.00         7\n",
      "        8311       0.00      0.00      0.00         4\n",
      "        8407       0.80      0.95      0.87        62\n",
      "        8409       0.00      0.00      0.00        49\n",
      "        8412       0.35      0.20      0.25        95\n",
      "        8413       0.51      0.89      0.65       123\n",
      "        8414       0.54      0.75      0.63       103\n",
      "        8415       0.00      0.00      0.00         7\n",
      "        8419       0.00      0.00      0.00        26\n",
      "        8421       0.46      0.89      0.61       124\n",
      "        8422       0.00      0.00      0.00         8\n",
      "        8423       0.74      0.81      0.77        48\n",
      "        8424       0.84      0.39      0.53        41\n",
      "        8425       1.00      0.39      0.57        33\n",
      "        8426       0.00      0.00      0.00        11\n",
      "        8427       0.00      0.00      0.00         4\n",
      "        8428       0.00      0.00      0.00        30\n",
      "        8429       0.00      0.00      0.00        15\n",
      "        8431       1.00      0.05      0.10        58\n",
      "        8443       0.00      0.00      0.00        16\n",
      "        8452       0.00      0.00      0.00        20\n",
      "        8454       0.00      0.00      0.00        15\n",
      "        8456       0.00      0.00      0.00        16\n",
      "        8462       0.00      0.00      0.00        28\n",
      "        8463       0.00      0.00      0.00         5\n",
      "        8464       0.00      0.00      0.00         4\n",
      "        8465       0.00      0.00      0.00         8\n",
      "        8466       0.00      0.00      0.00        39\n",
      "        8467       0.54      0.36      0.43        61\n",
      "        8468       0.00      0.00      0.00        11\n",
      "        8470       0.00      0.00      0.00         8\n",
      "        8471       0.49      1.00      0.65        70\n",
      "        8474       0.24      0.21      0.22        66\n",
      "        8477       0.00      0.00      0.00        34\n",
      "        8479       0.23      0.29      0.26        89\n",
      "        8480       0.00      0.00      0.00         3\n",
      "        8481       0.55      0.91      0.69       210\n",
      "        8482       0.94      0.72      0.82        94\n",
      "        8483       0.45      0.91      0.60       234\n",
      "        8484       0.00      0.00      0.00        26\n",
      "        8487       0.00      0.00      0.00        27\n",
      "        8501       0.50      0.96      0.65        95\n",
      "        8502       1.00      0.71      0.83        48\n",
      "        8503       0.00      0.00      0.00        10\n",
      "        8504       1.00      0.25      0.39       118\n",
      "        8505       1.00      0.02      0.04        49\n",
      "        8506       1.00      0.07      0.14        27\n",
      "        8507       0.00      0.00      0.00        20\n",
      "        8508       0.00      0.00      0.00        25\n",
      "        8509       0.00      0.00      0.00        36\n",
      "        8510       0.00      0.00      0.00        28\n",
      "        8511       0.00      0.00      0.00        39\n",
      "        8512       0.60      0.91      0.73       175\n",
      "        8513       0.84      0.99      0.91        71\n",
      "        8515       0.00      0.00      0.00        12\n",
      "        8516       0.57      0.96      0.71       159\n",
      "        8517       0.00      0.00      0.00        36\n",
      "        8518       1.00      0.46      0.63        35\n",
      "        8519       0.00      0.00      0.00        13\n",
      "        8523       0.00      0.00      0.00        17\n",
      "        8525       1.00      0.66      0.79        32\n",
      "        8526       0.00      0.00      0.00        10\n",
      "        8527       0.64      0.65      0.65        46\n",
      "        8528       0.00      0.00      0.00        11\n",
      "        8529       0.00      0.00      0.00         3\n",
      "        8531       0.00      0.00      0.00        20\n",
      "        8533       0.00      0.00      0.00        32\n",
      "        8534       0.00      0.00      0.00        18\n",
      "        8536       0.38      0.92      0.54       212\n",
      "        8537       0.00      0.00      0.00        23\n",
      "        8538       0.00      0.00      0.00        35\n",
      "        8539       0.77      0.92      0.84       146\n",
      "        8541       0.00      0.00      0.00        21\n",
      "        8543       0.00      0.00      0.00        28\n",
      "        8544       0.87      0.94      0.91       144\n",
      "        8545       0.00      0.00      0.00        12\n",
      "        8547       0.00      0.00      0.00        17\n",
      "        8708       0.86      0.96      0.91      1190\n",
      "        8712       0.00      0.00      0.00        17\n",
      "        8714       0.86      0.97      0.91       184\n",
      "        8715       0.00      0.00      0.00        11\n",
      "        8716       0.00      0.00      0.00        18\n",
      "        8903       0.00      0.00      0.00         5\n",
      "        9004       1.00      0.76      0.86        37\n",
      "        9005       1.00      0.17      0.29        35\n",
      "        9011       0.00      0.00      0.00        13\n",
      "        9013       0.00      0.00      0.00        22\n",
      "        9016       0.00      0.00      0.00        13\n",
      "        9017       0.00      0.00      0.00        37\n",
      "        9018       0.81      0.71      0.75        41\n",
      "        9019       0.00      0.00      0.00        11\n",
      "        9022       0.00      0.00      0.00         4\n",
      "        9023       0.00      0.00      0.00        15\n",
      "        9024       0.00      0.00      0.00        10\n",
      "        9025       1.00      0.24      0.39        41\n",
      "        9026       0.00      0.00      0.00        16\n",
      "        9027       0.00      0.00      0.00        18\n",
      "        9028       0.00      0.00      0.00        16\n",
      "        9029       0.00      0.00      0.00        15\n",
      "        9030       0.00      0.00      0.00         7\n",
      "        9031       0.00      0.00      0.00        39\n",
      "        9032       0.00      0.00      0.00        34\n",
      "        9033       0.00      0.00      0.00        19\n",
      "        9102       0.00      0.00      0.00        14\n",
      "        9105       0.00      0.00      0.00         7\n",
      "        9107       0.00      0.00      0.00        13\n",
      "        9113       0.00      0.00      0.00        11\n",
      "        9207       0.90      0.90      0.90        31\n",
      "        9304       0.00      0.00      0.00        10\n",
      "        9401       0.81      0.98      0.88        85\n",
      "        9403       0.71      0.96      0.82       139\n",
      "        9404       0.00      0.00      0.00        17\n",
      "        9405       0.73      0.98      0.84       415\n",
      "        9503       0.93      1.00      0.96      1726\n",
      "        9504       0.97      0.92      0.94       163\n",
      "        9505       0.92      0.56      0.70        43\n",
      "        9506       0.73      0.98      0.84       222\n",
      "        9507       1.00      0.03      0.06        31\n",
      "        9602       0.00      0.00      0.00        13\n",
      "        9603       0.48      0.96      0.64       144\n",
      "        9604       0.00      0.00      0.00         5\n",
      "        9605       0.00      0.00      0.00         9\n",
      "        9606       0.00      0.00      0.00        14\n",
      "        9607       1.00      0.75      0.86        28\n",
      "        9608       0.95      0.35      0.51        54\n",
      "        9609       0.85      0.85      0.85        40\n",
      "        9610       0.00      0.00      0.00        13\n",
      "        9613       0.00      0.00      0.00        14\n",
      "        9615       0.00      0.00      0.00        18\n",
      "        9616       0.00      0.00      0.00        21\n",
      "        9617       1.00      0.23      0.37        22\n",
      "        9620       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.68     17209\n",
      "   macro avg       0.27      0.23      0.22     17209\n",
      "weighted avg       0.61      0.68      0.61     17209\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sincosxy/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "Label_encoder = preprocessing.LabelEncoder()\n",
    "Label_encoder.classes_ = np.load('./classes.npy', allow_pickle=True)\n",
    "\n",
    "true = []\n",
    "preds = []\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(test_dataloader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    true += batch[\"labels\"].detach().cpu().numpy().tolist()\n",
    "    preds += predictions.detach().cpu().numpy().tolist()\n",
    "\n",
    "print(classification_report(Label_encoder.inverse_transform(true), Label_encoder.inverse_transform(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ee33c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('–ø–æ–∫—Ä—ã—à–∫–∏', truncation = True, max_length=100, padding='max_length', return_tensors=\"pt\")\n",
    "#batch = {k: torch.tensor(v).to(device) for k, v in txt.items()}\n",
    "model.to(torch.device('cpu'))\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "predicted_class_id = logits.argmax().item()\n",
    "#Label_encoder.inverse_transform(predicted_class_id)\n",
    "a = []\n",
    "a.append(predicted_class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "95f5252c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4011'], dtype=object)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Label_encoder.inverse_transform([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2efb846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(text):\n",
    "    model.to(torch.device('cpu'))\n",
    "    inputs = tokenizer(text, truncation = True, max_length=100, padding='max_length', return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "        return Label_encoder.inverse_transform([predicted_class_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a15ea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9503'] ['9503'] ['8714']\n"
     ]
    }
   ],
   "source": [
    "print(predict_class(\"–í–µ–ª–æ—Å–∏–ø–µ–¥\"), predict_class(\"–≤–µ–ª–æ—Å–∏–ø–µ–¥\"), predict_class(\"–í–µ–ª–æ—Å–∏–ø–µ–¥–æ–≤\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
